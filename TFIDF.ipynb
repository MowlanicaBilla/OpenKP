{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFIDF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlOR_OHmdxXF",
        "outputId": "ffbb49bf-ffe4-4373-dfcc-d727de77c466",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import operator\n",
        "import six\n",
        "from six.moves import range\n",
        "import math\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "stopwords = stopwords.words('english')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkUdcWJzIkLr"
      },
      "source": [
        "def leaves(tree):\n",
        "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
        "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
        "        yield subtree.leaves()\n",
        "\n",
        "def normalise(word):\n",
        "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
        "    word = word.lower()\n",
        "    #word = stemmer.stem(word)\n",
        "    #word = lemmatizer.lemmatize(word)\n",
        "    return word\n",
        "\n",
        "def acceptable_word(word):\n",
        "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
        "    accepted = bool(2 <= len(word) <= 40\n",
        "        and word.lower() not in stopwords)\n",
        "    return accepted\n",
        "\n",
        "\n",
        "def get_terms(tree):\n",
        "    for leaf in leaves(tree):\n",
        "        term = [ normalise(w) for w,t in leaf if acceptable_word(w) ]\n",
        "        yield term"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3-4L86Yd-Bp"
      },
      "source": [
        "text = \"\"\"Artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals.\"\"\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD3CDIpexd4L",
        "outputId": "3de90616-e59c-442e-fa36-686b9cdd5ce9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text.split(' ')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Artificial',\n",
              " 'intelligence',\n",
              " '(AI),',\n",
              " 'sometimes',\n",
              " 'called',\n",
              " 'machine',\n",
              " 'intelligence,',\n",
              " 'is',\n",
              " 'intelligence',\n",
              " 'demonstrated',\n",
              " 'by',\n",
              " 'machines,',\n",
              " 'unlike',\n",
              " 'the',\n",
              " 'natural',\n",
              " 'intelligence',\n",
              " 'displayed',\n",
              " 'by',\n",
              " 'humans',\n",
              " 'and',\n",
              " 'animals.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh37LtHrdxTs"
      },
      "source": [
        "# Used when tokenizing words\n",
        "sentence_re = r'''(?x)      # set flag to allow verbose regexps\n",
        "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
        "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
        "      | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
        "      | \\.\\.\\.              # ellipsis\n",
        "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
        "    '''\n",
        "\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "stemmer = nltk.stem.porter.PorterStemmer()\n",
        "\n",
        "#Taken from Su Nam Kim Paper\n",
        "grammar = r\"\"\"\n",
        "    NBAR:\n",
        "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
        "        \n",
        "    NP:\n",
        "        {<NBAR>}\n",
        "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
        "\"\"\"\n",
        "\n",
        "toks = nltk.regexp_tokenize(text, sentence_re)\n",
        "postoks = nltk.tag.pos_tag(toks)\n",
        "chunker = nltk.RegexpParser(grammar)\n",
        "tree = chunker.parse(postoks)\n",
        "terms = get_terms(tree)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PzfQ-tuX_jw"
      },
      "source": [
        "words = []\n",
        "for term in terms:\n",
        "  term1 = ' '.join([str(elem) for elem in term])\n",
        "  words.append(term1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga4DNjcUy9zz",
        "outputId": "a7b75d06-7d35-4906-aa96-848d56d39dc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "words"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['artificial intelligence',\n",
              " 'ai',\n",
              " 'machine intelligence',\n",
              " 'intelligence',\n",
              " 'machines',\n",
              " 'natural intelligence',\n",
              " 'humans',\n",
              " 'animals']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-CEkmJ5ukyX"
      },
      "source": [
        "word_embed = []\n",
        "for term in terms:\n",
        "  term1 = ' '.join([str(elem) for elem in term])\n",
        "  print(term1)\n",
        "  word_embed.append(get_w2v(term1,model))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM7cEH1xdxFz"
      },
      "source": [
        "# embed_dict = dict({})\n",
        "\n",
        "# for key,term in enumerate(terms):\n",
        "#     term1 = ' '.join([str(elem) for elem in term])\n",
        "#     embed_dict[key] = get_w2v(term1,model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EvyVhFbR594"
      },
      "source": [
        "#embed_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfDRVsy3SOpZ"
      },
      "source": [
        "## GloVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SodDHkh4dxDZ",
        "outputId": "6d033509-b442-4375-c2b8-3528d8a2be1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-16 15:19:03--  http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
            "--2020-11-16 15:19:03--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2020-11-16 15:19:04--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  1.98MB/s    in 16m 54s \n",
            "\n",
            "2020-11-16 15:35:58 (2.05 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "914uUiNFdxAr",
        "outputId": "4bd5d8cc-5419-41f4-e58d-34e7049224b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip glove*.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.840B.300d.zip\n",
            "  inflating: glove.840B.300d.txt     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x77cFNhzZb0f"
      },
      "source": [
        "def load_glove_model(glove_file):\n",
        "  model = {}\n",
        "  f = open(glove_file)\n",
        "  for line in f:\n",
        "    values = line.split(' ')\n",
        "    word = values[0] ## The first entry is the word\n",
        "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
        "    model[word] = coefs\n",
        "  return model\n",
        "  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0OVdjTaZqMf"
      },
      "source": [
        "model = load_glove_model('/content/glove.840B.300d.txt')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PFVQhI5aB_8"
      },
      "source": [
        "def get_w2v(sentence, model):\n",
        "    \"\"\"\n",
        "    :param sentence: inputs a single sentences whose word embedding is to be extracted.\n",
        "    :param model: inputs glove model.\n",
        "    :return: returns numpy array containing word embedding of all words    in input sentence.\n",
        "    \"\"\"\n",
        "    return np.array([model.get(val, np.zeros(100)) for val in sentence.split()], dtype=np.float64)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_gApGuxaERB"
      },
      "source": [
        "ai_kw = get_w2v(\"artificial intelligence\", model)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frI6zIEgWyKu",
        "outputId": "488918a8-636e-42b7-b4dc-3ca33988eb9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ai_kw.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dcy-UMsvOWg",
        "outputId": "50d12ad8-4e86-4e9b-a2d0-60f6c6909019",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for word in words:\n",
        "  result = [i+1 for i,w in enumerate(text.split()) if w.lower() == word]\n",
        "  print(word,result)  "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "artificial intelligence []\n",
            "ai []\n",
            "machine intelligence []\n",
            "intelligence [2, 9, 16]\n",
            "machines []\n",
            "natural intelligence []\n",
            "humans [19]\n",
            "animals []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc2arO6TCrrn"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IownSpNyFsj"
      },
      "source": [
        "text =\"\"\"Keyphrases are capable of providing semantic metadata characterizing documents and producing an overview of the content of a document. Since keyphrase extraction is able to facilitate the management, categorization, and retrieval of information, it has received much attention in recent years. There are three approaches to address keyphrase extraction: (i) traditional two-step ranking method, (ii) sequence labeling and (iii) generation using neural networks. Two-step ranking approach is based on feature engineering, which is labor intensive and domain dependent. Sequence labeling is not able to tackle overlapping phrases. Generation methods (i.e., Sequence-to-sequence neural network models) overcome those shortcomings, so they have been widely studied and gain state-of-the-art performance. However, generation methods can not utilize context information effectively. In this paper, we propose a novelty Span Keyphrase Extraction model that extracts span-based feature representation of keyphrase directly from all the content tokens. In this way, our model obtains representation for each keyphrase and further learns to capture the interaction between keyphrases in one document to get better ranking results. In addition, with the help of tokens, our model is able to extract overlapped keyphrases. Experimental results on the benchmark datasets show that our proposed model outperforms the existing methods by a large margin\"\"\""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djhq2UCVyASZ",
        "outputId": "11fcb7e7-fbc9-4d3d-c12c-0a235410e701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "keywords = re.findall(r'[a-zA-Z]\\w+',text)\n",
        "len(keywords)   "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDMi9dvFFauT"
      },
      "source": [
        "stopwords.extend(['there'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TLVUdyADP7t"
      },
      "source": [
        "keywords = [word.lower() for word in keywords if not word in stopwords]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfCUdYzkDpJz",
        "outputId": "719e84b8-e600-42c5-86a7-ead8d1d75181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(keywords)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "137"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF5PwC1KyAOJ"
      },
      "source": [
        "df = pd.DataFrame(list(set(keywords)),columns=['keywords'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu5GRZmzB_xv"
      },
      "source": [
        "def weightage(word,text,number_of_documents=1):\n",
        "  try:\n",
        "    word_list = re.findall(word,text)\n",
        "    number_of_times_word_appeared =len(word_list)\n",
        "    tf = number_of_times_word_appeared/float(len(text))\n",
        "    idf = np.log((number_of_documents)/float(number_of_times_word_appeared))\n",
        "    tf_idf = tf*idf\n",
        "    return number_of_times_word_appeared,tf,idf ,tf_idf\n",
        "  except ZeroDivisionError:\n",
        "    return '0000'"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evZRYmaIvOcc"
      },
      "source": [
        "df['number_of_times_word_appeared'] = df['keywords'].apply(lambda x: weightage(x,text)[0])\n",
        "df['tf'] = df['keywords'].apply(lambda x: weightage(x,text)[1])\n",
        "df['idf'] = df['keywords'].apply(lambda x: weightage(x,text)[2])\n",
        "df['tf_idf'] = df['keywords'].apply(lambda x: weightage(x,text)[3])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rihiBWi3GUig"
      },
      "source": [
        "df.tf = df.tf.astype(int)\n",
        "df.idf = df.idf.astype(int)\n",
        "df.tf_idf = df.tf_idf.astype(int)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arkINqiivOS3"
      },
      "source": [
        "df = df.sort_values('tf_idf',ascending=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOLWdYAYvn3a",
        "outputId": "d069d36e-1bb8-4e96-d955-56e2df2091ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keywords</th>\n",
              "      <th>number_of_times_word_appeared</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "      <th>tf_idf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>show</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>two</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>phrases</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>producing</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>there</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>representation</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>effectively</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>tokens</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>proposed</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>ranking</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          keywords number_of_times_word_appeared  tf  idf  tf_idf\n",
              "0             show                             1   0    0       0\n",
              "72             two                             3   0   -1       0\n",
              "71         phrases                             4   0   -1       0\n",
              "70       producing                             1   0    0       0\n",
              "69           there                             0   0    0       0\n",
              "..             ...                           ...  ..  ...     ...\n",
              "28  representation                             2   0    0       0\n",
              "27     effectively                             1   0    0       0\n",
              "26          tokens                             2   0    0       0\n",
              "36        proposed                             1   0    0       0\n",
              "99         ranking                             3   0   -1       0\n",
              "\n",
              "[100 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To1qtyGiwuLD"
      },
      "source": [
        "from gensim.summarization import keywords"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w04ihdvWw_3P"
      },
      "source": [
        "text = lemmatizer.lemmatize(text)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03BbVidTBkZx"
      },
      "source": [
        "values = keywords(text=text,split='\\n',scores=True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlF-PIT_o5ya",
        "outputId": "09f88fc7-4a4e-418f-9f62-c9ba070679a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "values"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('models', 0.3001112501765992),\n",
              " ('model', 0.3001112501765992),\n",
              " ('keyphrases', 0.27404194456014663),\n",
              " ('ranking', 0.27283931746305884),\n",
              " ('keyphrase extraction', 0.22615620697286487),\n",
              " ('overlapping', 0.1867938208832368),\n",
              " ('span', 0.1866519651190099),\n",
              " ('extract overlapped', 0.18253214513440996),\n",
              " ('sequence', 0.18057995267706373),\n",
              " ('extracts', 0.17827046938558314),\n",
              " ('method', 0.16435153680784498),\n",
              " ('methods', 0.16435153680784498),\n",
              " ('generation', 0.1619082838963352),\n",
              " ('feature', 0.16104941841724807),\n",
              " ('context', 0.15210999120751884),\n",
              " ('information', 0.15210999120751875)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BEMzD6tBkgt"
      },
      "source": [
        "data = pd.DataFrame(values,columns=['keyword','score'])\n",
        "data = data.sort_values('score',ascending=False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxV7t2RwBkj0",
        "outputId": "d517b3c1-5495-41cf-b824-30f01fc830e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keyword</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>models</td>\n",
              "      <td>0.300111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>model</td>\n",
              "      <td>0.300111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>keyphrases</td>\n",
              "      <td>0.274042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ranking</td>\n",
              "      <td>0.272839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>keyphrase extraction</td>\n",
              "      <td>0.226156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>overlapping</td>\n",
              "      <td>0.186794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>span</td>\n",
              "      <td>0.186652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>extract overlapped</td>\n",
              "      <td>0.182532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>sequence</td>\n",
              "      <td>0.180580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>extracts</td>\n",
              "      <td>0.178270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>method</td>\n",
              "      <td>0.164352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>methods</td>\n",
              "      <td>0.164352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>generation</td>\n",
              "      <td>0.161908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>feature</td>\n",
              "      <td>0.161049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>context</td>\n",
              "      <td>0.152110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>information</td>\n",
              "      <td>0.152110</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 keyword     score\n",
              "0                 models  0.300111\n",
              "1                  model  0.300111\n",
              "2             keyphrases  0.274042\n",
              "3                ranking  0.272839\n",
              "4   keyphrase extraction  0.226156\n",
              "5            overlapping  0.186794\n",
              "6                   span  0.186652\n",
              "7     extract overlapped  0.182532\n",
              "8               sequence  0.180580\n",
              "9               extracts  0.178270\n",
              "10                method  0.164352\n",
              "11               methods  0.164352\n",
              "12            generation  0.161908\n",
              "13               feature  0.161049\n",
              "14               context  0.152110\n",
              "15           information  0.152110"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xpv88z4c_3K"
      },
      "source": [
        "## Keyphrases based on Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RaxaiULZea4"
      },
      "source": [
        "# Required functions for RAKE\n",
        "def is_number(s):\n",
        "    try:\n",
        "        float(s) if '.' in s else int(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def load_stop_words(stop_word_file):\n",
        "    \"\"\"\n",
        "    Utility function to load stop words from a file and return as a list of words\n",
        "    @param stop_word_file Path and file name of a file containing stop words.\n",
        "    @return list A list of stop words.\n",
        "    \"\"\"\n",
        "    stop_words = []\n",
        "    for line in open(stop_word_file):\n",
        "        if line.strip()[0:1] != \"#\":\n",
        "            for word in line.split():  # in case more than one per line\n",
        "                stop_words.append(word)\n",
        "    return stop_words\n",
        "\n",
        "\n",
        "def separate_words(text, min_word_return_size):\n",
        "    \"\"\"\n",
        "    Utility function to return a list of all words that are have a length greater than a specified number of characters.\n",
        "    @param text The text that must be split in to words.\n",
        "    @param min_word_return_size The minimum no of characters a word must have to be included.\n",
        "    \"\"\"\n",
        "    splitter = re.compile('[^a-zA-Z0-9_\\\\+\\\\-/]')\n",
        "    words = []\n",
        "    for single_word in splitter.split(text):\n",
        "        current_word = single_word.strip().lower()\n",
        "        #leave numbers in phrase, but don't count as words, since they tend to invalidate scores of their phrases\n",
        "        if len(current_word) > min_word_return_size and current_word != '' and not is_number(current_word):\n",
        "            words.append(current_word)\n",
        "    return words\n",
        "\n",
        "\n",
        "def split_sentences(text):\n",
        "    \"\"\"\n",
        "    Utility function to return a list of sentences.\n",
        "    @param text The text that must be split in to sentences.\n",
        "    \"\"\"\n",
        "    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?,;:\\t\\\\-\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013]')\n",
        "    sentences = sentence_delimiters.split(text)\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def build_stop_word_regex(stop_word_file_path):\n",
        "    stop_word_list = load_stop_words(stop_word_file_path)\n",
        "    stop_word_regex_list = []\n",
        "    for word in stop_word_list:\n",
        "        word_regex = '\\\\b' + word + '\\\\b'\n",
        "        stop_word_regex_list.append(word_regex)\n",
        "    stop_word_pattern = re.compile('|'.join(stop_word_regex_list), re.IGNORECASE)\n",
        "    return stop_word_pattern\n",
        "\n",
        "\n",
        "def generate_candidate_keywords(sentence_list, stopword_pattern, min_char_length=1, max_words_length=5):\n",
        "    phrase_list = []\n",
        "    for s in sentence_list:\n",
        "        tmp = re.sub(stopword_pattern, '|', s.strip())\n",
        "        phrases = tmp.split(\"|\")\n",
        "        for phrase in phrases:\n",
        "            phrase = phrase.strip().lower()\n",
        "            if phrase != \"\" and is_acceptable(phrase, min_char_length, max_words_length):\n",
        "                phrase_list.append(phrase)\n",
        "    return phrase_list\n",
        "\n",
        "\n",
        "def is_acceptable(phrase, min_char_length, max_words_length):\n",
        "\n",
        "    # a phrase must have a min length in characters\n",
        "    if len(phrase) < min_char_length:\n",
        "        return 0\n",
        "\n",
        "    # a phrase must have a max number of words\n",
        "    words = phrase.split()\n",
        "    if len(words) > max_words_length:\n",
        "        return 0\n",
        "\n",
        "    digits = 0\n",
        "    alpha = 0\n",
        "    for i in range(0, len(phrase)):\n",
        "        if phrase[i].isdigit():\n",
        "            digits += 1\n",
        "        elif phrase[i].isalpha():\n",
        "            alpha += 1\n",
        "\n",
        "    # a phrase must have at least one alpha character\n",
        "    if alpha == 0:\n",
        "        return 0\n",
        "\n",
        "    # a phrase must have more alpha than digits characters\n",
        "    if digits > alpha:\n",
        "        return 0\n",
        "    return 1\n",
        "\n",
        "\n",
        "def calculate_word_scores(phraseList):\n",
        "    word_frequency = {}\n",
        "    word_degree = {}\n",
        "    for phrase in phraseList:\n",
        "        word_list = separate_words(phrase, 0)\n",
        "        word_list_length = len(word_list)\n",
        "        word_list_degree = word_list_length - 1\n",
        "        # if word_list_degree > 3: word_list_degree = 3 #exp.\n",
        "        for word in word_list:\n",
        "            word_frequency.setdefault(word, 0)\n",
        "            word_frequency[word] += 1\n",
        "            word_degree.setdefault(word, 0)\n",
        "            word_degree[word] += word_list_degree  # orig.\n",
        "            # word_degree[word] += 1/(word_list_length*1.0) #exp.\n",
        "    for item in word_frequency:\n",
        "        word_degree[item] = word_degree[item] + word_frequency[item]\n",
        "\n",
        "    # Calculate Word scores = deg(w)/freq(w)\n",
        "    word_score = {}\n",
        "    for item in word_frequency:\n",
        "        word_score.setdefault(item, 0)\n",
        "        word_score[item] = word_degree[item] / (word_frequency[item] * 1.0)  #orig.\n",
        "    # word_score[item] = word_frequency[item]/(word_degree[item] * 1.0) #exp.\n",
        "    return word_score\n",
        "\n",
        "\n",
        "def generate_candidate_keyword_scores(phrase_list, word_score, min_keyword_frequency=2):\n",
        "    keyword_candidates = {}\n",
        "\n",
        "    for phrase in phrase_list:\n",
        "        if min_keyword_frequency > 2:\n",
        "            if phrase_list.count(phrase) < min_keyword_frequency:\n",
        "                continue\n",
        "        keyword_candidates.setdefault(phrase, 0)\n",
        "        word_list = separate_words(phrase, 0)\n",
        "        candidate_score = 0\n",
        "        for word in word_list:\n",
        "            candidate_score += word_score[word]\n",
        "        keyword_candidates[phrase] = candidate_score\n",
        "    return keyword_candidates\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouJpRiehcI8k"
      },
      "source": [
        "def build_stop_word_regex(stop_word_file_path):\n",
        "    stop_word_list = load_stop_words(stop_word_file_path)\n",
        "    stop_word_regex_list = []\n",
        "    for word in stop_word_list:\n",
        "        word_regex = '\\\\b' + word + '\\\\b'\n",
        "        stop_word_regex_list.append(word_regex)\n",
        "    stop_word_pattern = re.compile('|'.join(stop_word_regex_list), re.IGNORECASE)\n",
        "    return stop_word_pattern"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnABboVWcLuE"
      },
      "source": [
        "stopword_pattern = build_stop_word_regex('/content/drive/My Drive/Colab Notebooks/Advanced NLP-Project/Project/sklearn_stopwords.txt')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj-YFyq6ce-t"
      },
      "source": [
        "def separate_words(text, min_word_return_size):\n",
        "    \"\"\"\n",
        "    Utility function to return a list of all words that are have a length greater than a specified number of characters.\n",
        "    @param text The text that must be split in to words.\n",
        "    @param min_word_return_size The minimum no of characters a word must have to be included.\n",
        "    \"\"\"\n",
        "    splitter = re.compile('[^a-zA-Z0-9_\\\\+\\\\-/]')\n",
        "    words = []\n",
        "    for single_word in splitter.split(text):\n",
        "        current_word = single_word.strip().lower()\n",
        "        #leave numbers in phrase, but don't count as words, since they tend to invalidate scores of their phrases\n",
        "        if len(current_word) > min_word_return_size and current_word != '' and not is_number(current_word):\n",
        "            words.append(current_word)\n",
        "    return words"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeqS-jOYcZiH"
      },
      "source": [
        "def generate_candidate_keywords(sentence_list, stopword_pattern, min_char_length=1, max_words_length=5):\n",
        "    phrase_list = []\n",
        "    for s in sentence_list:\n",
        "        tmp = re.sub(stopword_pattern, '|', s.strip())\n",
        "        phrases = tmp.split(\"|\")\n",
        "        for phrase in phrases:\n",
        "            phrase = phrase.strip().lower()\n",
        "            if phrase != \"\" and is_acceptable(phrase, min_char_length, max_words_length):\n",
        "                phrase_list.append(phrase)\n",
        "    return phrase_list"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSAwX21aJ0xa"
      },
      "source": [
        "text = \"\"\"A challenging problem faced by researchers and developers\n",
        "of distributed real-time and embedded (DRE) systems is \n",
        "devising and implementing effective adaptive resource \n",
        "management strategies that can meet end-to-end quality of service\n",
        "(QoS) requirements in varying operational conditions. This\n",
        "paper presents two contributions to research in adaptive \n",
        "resource management for DRE systems. First, we describe the\n",
        "structure and functionality of the Hybrid Adaptive \n",
        "Resourcemanagement Middleware (HyARM), which provides \n",
        "adaptive resource management using hybrid control techniques\n",
        "for adapting to workload fluctuations and resource \n",
        "availability. Second, we evaluate the adaptive behavior of HyARM\n",
        "via experiments on a DRE multimedia system that distributes\n",
        "video in real-time. Our results indicate that HyARM yields\n",
        "predictable, stable, and high system performance, even in the\n",
        "face of fluctuating workload and resource availability\"\"\""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXSBmXUJclJU"
      },
      "source": [
        "word_list = separate_words(text,3)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0PBXA1tca28"
      },
      "source": [
        "phraseList = generate_candidate_keywords(sentence_list=word_list,stopword_pattern=stopword_pattern)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po-kavTXKQjh",
        "outputId": "d7ce3225-f2b9-4a8b-f037-d9da5f6dc2de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "phraseList"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['challenging',\n",
              " 'problem',\n",
              " 'faced',\n",
              " 'researchers',\n",
              " 'developers',\n",
              " 'distributed',\n",
              " 'real-time',\n",
              " 'embedded',\n",
              " 'systems',\n",
              " 'devising',\n",
              " 'implementing',\n",
              " 'effective',\n",
              " 'adaptive',\n",
              " 'resource',\n",
              " 'management',\n",
              " 'strategies',\n",
              " 'meet',\n",
              " 'end-',\n",
              " '-end',\n",
              " 'quality',\n",
              " 'service',\n",
              " 'requirements',\n",
              " 'varying',\n",
              " 'operational',\n",
              " 'conditions',\n",
              " 'paper',\n",
              " 'presents',\n",
              " 'contributions',\n",
              " 'research',\n",
              " 'adaptive',\n",
              " 'resource',\n",
              " 'management',\n",
              " 'systems',\n",
              " 'structure',\n",
              " 'functionality',\n",
              " 'hybrid',\n",
              " 'adaptive',\n",
              " 'resourcemanagement',\n",
              " 'middleware',\n",
              " 'hyarm',\n",
              " 'provides',\n",
              " 'adaptive',\n",
              " 'resource',\n",
              " 'management',\n",
              " 'using',\n",
              " 'hybrid',\n",
              " 'control',\n",
              " 'techniques',\n",
              " 'adapting',\n",
              " 'workload',\n",
              " 'fluctuations',\n",
              " 'resource',\n",
              " 'availability',\n",
              " 'second',\n",
              " 'evaluate',\n",
              " 'adaptive',\n",
              " 'behavior',\n",
              " 'hyarm',\n",
              " 'experiments',\n",
              " 'multimedia',\n",
              " 'distributes',\n",
              " 'video',\n",
              " 'real-time',\n",
              " 'results',\n",
              " 'indicate',\n",
              " 'hyarm',\n",
              " 'yields',\n",
              " 'predictable',\n",
              " 'stable',\n",
              " 'high',\n",
              " 'performance',\n",
              " 'face',\n",
              " 'fluctuating',\n",
              " 'workload',\n",
              " 'resource',\n",
              " 'availability']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynWVcdzkec_Z"
      },
      "source": [
        "word_score = calculate_word_scores(phraseList=phraseList)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPbo7ZDieQmC",
        "outputId": "f2f6e62d-a99e-44b5-f7ad-40dc9bd69fdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "generate_candidate_keyword_scores(phrase_list=phraseList, word_score=word_score)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'-end': 1.0,\n",
              " 'adapting': 1.0,\n",
              " 'adaptive': 1.0,\n",
              " 'availability': 1.0,\n",
              " 'behavior': 1.0,\n",
              " 'challenging': 1.0,\n",
              " 'conditions': 1.0,\n",
              " 'contributions': 1.0,\n",
              " 'control': 1.0,\n",
              " 'developers': 1.0,\n",
              " 'devising': 1.0,\n",
              " 'distributed': 1.0,\n",
              " 'distributes': 1.0,\n",
              " 'effective': 1.0,\n",
              " 'embedded': 1.0,\n",
              " 'end-': 1.0,\n",
              " 'evaluate': 1.0,\n",
              " 'experiments': 1.0,\n",
              " 'face': 1.0,\n",
              " 'faced': 1.0,\n",
              " 'fluctuating': 1.0,\n",
              " 'fluctuations': 1.0,\n",
              " 'functionality': 1.0,\n",
              " 'high': 1.0,\n",
              " 'hyarm': 1.0,\n",
              " 'hybrid': 1.0,\n",
              " 'implementing': 1.0,\n",
              " 'indicate': 1.0,\n",
              " 'management': 1.0,\n",
              " 'meet': 1.0,\n",
              " 'middleware': 1.0,\n",
              " 'multimedia': 1.0,\n",
              " 'operational': 1.0,\n",
              " 'paper': 1.0,\n",
              " 'performance': 1.0,\n",
              " 'predictable': 1.0,\n",
              " 'presents': 1.0,\n",
              " 'problem': 1.0,\n",
              " 'provides': 1.0,\n",
              " 'quality': 1.0,\n",
              " 'real-time': 1.0,\n",
              " 'requirements': 1.0,\n",
              " 'research': 1.0,\n",
              " 'researchers': 1.0,\n",
              " 'resource': 1.0,\n",
              " 'resourcemanagement': 1.0,\n",
              " 'results': 1.0,\n",
              " 'second': 1.0,\n",
              " 'service': 1.0,\n",
              " 'stable': 1.0,\n",
              " 'strategies': 1.0,\n",
              " 'structure': 1.0,\n",
              " 'systems': 1.0,\n",
              " 'techniques': 1.0,\n",
              " 'using': 1.0,\n",
              " 'varying': 1.0,\n",
              " 'video': 1.0,\n",
              " 'workload': 1.0,\n",
              " 'yields': 1.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fM5SNz3gds0"
      },
      "source": [
        "def get_w2v(sentence, model):\n",
        "  feature_vec = np.array([model.get(val, np.zeros(300)) for val in sentence.split()], dtype=np.float64)\n",
        "  return feature_vec"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwSOakvyf4_u",
        "outputId": "a780cf15-2467-4769-aa56-7d2fd39ee7c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "get_w2v(\"artificial\",model)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.66058999,  0.2348    , -0.021227  , -0.32736999, -0.062493  ,\n",
              "         0.27303001,  0.1733    ,  0.70947999,  0.21335   ,  1.48800004,\n",
              "        -0.34101999,  0.1902    , -0.37272999,  0.33818001,  0.052431  ,\n",
              "        -0.17522   , -0.30445999,  1.74199998,  0.068747  , -0.42475   ,\n",
              "         0.041578  ,  0.42822999, -0.098792  ,  0.35168999, -0.34538999,\n",
              "         0.04463   ,  0.078867  , -0.084303  ,  0.29864001, -0.46726999,\n",
              "        -0.24345   ,  0.12309   , -0.025794  , -0.48201001,  0.81326002,\n",
              "        -0.18169001,  0.44051   , -0.14026999,  0.3556    , -0.12732001,\n",
              "         0.20464   , -0.019575  , -0.0212    ,  0.42548001,  0.33680999,\n",
              "        -0.12704   , -0.32006001, -0.24986   ,  0.22744   ,  0.026694  ,\n",
              "        -0.19501001, -0.055653  , -0.47916999,  0.20393001,  0.037399  ,\n",
              "        -0.13944   , -0.12375   ,  0.35220999, -0.37195   , -0.42001   ,\n",
              "        -0.15995   , -0.34676999,  0.13706   ,  0.23802   ,  0.22725999,\n",
              "         0.37389001, -0.021143  , -0.75593001, -0.86084998, -0.91714001,\n",
              "        -0.13774   ,  0.41578999, -0.051348  ,  0.21478   ,  0.018621  ,\n",
              "         0.14681999, -0.23421   ,  0.35569   ,  0.085372  , -0.68581998,\n",
              "         0.059522  ,  0.50234002, -0.36945999, -0.20005   , -0.071272  ,\n",
              "        -0.68017   , -0.017398  ,  1.36849999, -0.58291   , -0.61517   ,\n",
              "        -0.37472999,  0.67935997, -0.82725   , -0.27474999, -0.0079224 ,\n",
              "         0.36451   ,  0.211     ,  0.43897   , -0.0654    , -0.28604001,\n",
              "         0.020516  ,  0.53808999, -0.093647  , -0.33568001,  0.26813999,\n",
              "        -2.05270004, -0.1012    ,  0.17515001,  0.19621   ,  0.052598  ,\n",
              "        -0.16722   ,  0.28806999, -0.21741   ,  0.16098   , -0.10056   ,\n",
              "        -0.23423   , -0.040452  , -0.062229  ,  0.096816  , -0.24483   ,\n",
              "         0.31524   ,  0.36434999, -0.003012  , -0.17199001, -0.34577999,\n",
              "        -0.13416   ,  0.63941002,  0.69924998,  0.73940998,  0.79161   ,\n",
              "         0.0446    , -0.069197  , -0.21889   , -0.15790001,  0.62066001,\n",
              "        -0.37191001,  0.22973   , -0.026323  ,  0.26596001, -0.14167   ,\n",
              "        -0.68849999,  0.61342001,  0.30489001,  0.43177   , -0.28240001,\n",
              "        -0.56761003, -0.61315   , -0.57670999, -0.32712999, -0.079609  ,\n",
              "         0.41714001, -0.066882  , -0.20646   , -0.34812   ,  0.078564  ,\n",
              "         0.15966   ,  0.20471001, -0.33362001,  0.11944   ,  0.22239   ,\n",
              "         0.2075    , -0.73815   , -0.02446   ,  0.37976   , -0.04409   ,\n",
              "         0.12194   ,  0.30375999,  0.28797999,  0.12638   ,  0.74491   ,\n",
              "         0.14765   , -0.053074  , -0.22510999, -0.014963  ,  0.084669  ,\n",
              "         0.12157   ,  0.099844  , -0.4206    ,  0.46840999,  0.067702  ,\n",
              "         0.48137   , -0.14936   ,  0.12552001, -0.78369999,  0.05704   ,\n",
              "        -0.25193   ,  0.32098001, -0.069512  , -0.30511999, -0.32234001,\n",
              "        -0.19796   , -0.15263   , -0.0053759 , -0.16266   , -0.38712999,\n",
              "         0.47758999,  0.062617  ,  0.28077999, -0.098818  ,  0.99344999,\n",
              "        -0.32907999,  0.30434   ,  0.38409999,  0.24823   , -0.26853001,\n",
              "         0.22131   ,  0.52609003, -0.093188  ,  0.65504998, -0.077682  ,\n",
              "        -0.37237999, -0.080269  ,  0.45903999, -0.77616   , -0.019988  ,\n",
              "        -0.028252  ,  0.31957   , -0.18246   , -0.10505   , -0.85017002,\n",
              "        -0.0043364 ,  0.46678001, -0.11024   , -0.42473   ,  0.17364   ,\n",
              "         0.22729   ,  0.23514   ,  0.057877  ,  0.03433   ,  0.19267   ,\n",
              "         0.93722999,  0.23856001,  0.25615001, -0.20395   , -0.11687   ,\n",
              "         0.45027   ,  0.20274   , -0.074019  , -0.097887  , -0.081815  ,\n",
              "        -0.10759   ,  0.06972   , -0.74190003,  0.087821  , -0.44632   ,\n",
              "         0.30441999, -0.065095  , -0.10194   , -0.24260999,  0.44082999,\n",
              "        -0.50321001, -0.24467   ,  0.49724999, -0.019037  ,  0.29782999,\n",
              "        -0.20172   , -0.11587   ,  0.47088999,  0.49215001, -0.28443   ,\n",
              "         0.32115999, -0.39655   ,  0.30941999,  0.30212   , -0.23395   ,\n",
              "         0.56835002,  0.050275  , -0.18996   ,  0.22544999, -0.1401    ,\n",
              "        -0.23089001,  0.69198   ,  0.16433001,  0.52161998, -0.12018   ,\n",
              "        -0.25613001, -0.47067001, -0.12959   , -0.52708   , -0.29701999,\n",
              "         0.28163999, -0.86044002,  0.26973   , -0.98232001, -0.41771001,\n",
              "        -0.39168999, -0.2642    , -0.53934002, -0.12058   , -0.38424999,\n",
              "        -0.070265  ,  0.042661  , -0.71386999, -0.39471   , -0.20822001,\n",
              "         0.14079   , -0.065129  ,  0.098778  ,  0.080966  ,  0.32161   ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42fs9oDMOKlV"
      },
      "source": [
        "## TextRank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmP0qaGzOJvZ",
        "outputId": "7b5a8acc-449c-4648-eee9-588b43a60818",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def clean(text):\n",
        "    text = text.lower()\n",
        "    printable = set(string.printable)\n",
        "    text = filter(lambda x: x in printable, text)\n",
        "    text = \"\".join(list(text))\n",
        "    return text\n",
        "\n",
        "Cleaned_text = clean(text)\n",
        "# print(Cleaned_text)\n",
        "text = word_tokenize(Cleaned_text)\n",
        "\n",
        "print (\"Tokenized Text: \\n\")\n",
        "print (text)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized Text: \n",
            "\n",
            "['a', 'challenging', 'problem', 'faced', 'by', 'researchers', 'and', 'developers', 'of', 'distributed', 'real-time', 'and', 'embedded', '(', 'dre', ')', 'systems', 'is', 'devising', 'and', 'implementing', 'effective', 'adaptive', 'resource', 'management', 'strategies', 'that', 'can', 'meet', 'end-to-end', 'quality', 'of', 'service', '(', 'qos', ')', 'requirements', 'in', 'varying', 'operational', 'conditions', '.', 'this', 'paper', 'presents', 'two', 'contributions', 'to', 'research', 'in', 'adaptive', 'resource', 'management', 'for', 'dre', 'systems', '.', 'first', ',', 'we', 'describe', 'the', 'structure', 'and', 'functionality', 'of', 'the', 'hybrid', 'adaptive', 'resourcemanagement', 'middleware', '(', 'hyarm', ')', ',', 'which', 'provides', 'adaptive', 'resource', 'management', 'using', 'hybrid', 'control', 'techniques', 'for', 'adapting', 'to', 'workload', 'fluctuations', 'and', 'resource', 'availability', '.', 'second', ',', 'we', 'evaluate', 'the', 'adaptive', 'behavior', 'of', 'hyarm', 'via', 'experiments', 'on', 'a', 'dre', 'multimedia', 'system', 'that', 'distributes', 'video', 'in', 'real-time', '.', 'our', 'results', 'indicate', 'that', 'hyarm', 'yields', 'predictable', ',', 'stable', ',', 'and', 'high', 'system', 'performance', ',', 'even', 'in', 'the', 'face', 'of', 'fluctuating', 'workload', 'and', 'resource', 'availability']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwRyq0d2OSui",
        "outputId": "9dd6ec2a-7c92-4bae-9e4a-58685be8ccfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pos_tag = nltk.pos_tag(text)\n",
        "\n",
        "print (\"Tokenized Text with POS tags: \\n\")\n",
        "print (pos_tag)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized Text with POS tags: \n",
            "\n",
            "[('a', 'DT'), ('challenging', 'NN'), ('problem', 'NN'), ('faced', 'VBN'), ('by', 'IN'), ('researchers', 'NNS'), ('and', 'CC'), ('developers', 'NNS'), ('of', 'IN'), ('distributed', 'VBN'), ('real-time', 'NN'), ('and', 'CC'), ('embedded', 'VBD'), ('(', '('), ('dre', 'NN'), (')', ')'), ('systems', 'NNS'), ('is', 'VBZ'), ('devising', 'VBG'), ('and', 'CC'), ('implementing', 'VBG'), ('effective', 'JJ'), ('adaptive', 'JJ'), ('resource', 'NN'), ('management', 'NN'), ('strategies', 'NNS'), ('that', 'WDT'), ('can', 'MD'), ('meet', 'VB'), ('end-to-end', 'JJ'), ('quality', 'NN'), ('of', 'IN'), ('service', 'NN'), ('(', '('), ('qos', 'NN'), (')', ')'), ('requirements', 'NNS'), ('in', 'IN'), ('varying', 'VBG'), ('operational', 'JJ'), ('conditions', 'NNS'), ('.', '.'), ('this', 'DT'), ('paper', 'NN'), ('presents', 'VBZ'), ('two', 'CD'), ('contributions', 'NNS'), ('to', 'TO'), ('research', 'NN'), ('in', 'IN'), ('adaptive', 'JJ'), ('resource', 'NN'), ('management', 'NN'), ('for', 'IN'), ('dre', 'NN'), ('systems', 'NNS'), ('.', '.'), ('first', 'RB'), (',', ','), ('we', 'PRP'), ('describe', 'VBP'), ('the', 'DT'), ('structure', 'NN'), ('and', 'CC'), ('functionality', 'NN'), ('of', 'IN'), ('the', 'DT'), ('hybrid', 'JJ'), ('adaptive', 'JJ'), ('resourcemanagement', 'NN'), ('middleware', 'NN'), ('(', '('), ('hyarm', 'NN'), (')', ')'), (',', ','), ('which', 'WDT'), ('provides', 'VBZ'), ('adaptive', 'JJ'), ('resource', 'NN'), ('management', 'NN'), ('using', 'VBG'), ('hybrid', 'JJ'), ('control', 'NN'), ('techniques', 'NNS'), ('for', 'IN'), ('adapting', 'VBG'), ('to', 'TO'), ('workload', 'VB'), ('fluctuations', 'NNS'), ('and', 'CC'), ('resource', 'NN'), ('availability', 'NN'), ('.', '.'), ('second', 'JJ'), (',', ','), ('we', 'PRP'), ('evaluate', 'VBP'), ('the', 'DT'), ('adaptive', 'JJ'), ('behavior', 'NN'), ('of', 'IN'), ('hyarm', 'NN'), ('via', 'IN'), ('experiments', 'NNS'), ('on', 'IN'), ('a', 'DT'), ('dre', 'NN'), ('multimedia', 'NN'), ('system', 'NN'), ('that', 'WDT'), ('distributes', 'VBZ'), ('video', 'NN'), ('in', 'IN'), ('real-time', 'NN'), ('.', '.'), ('our', 'PRP$'), ('results', 'NNS'), ('indicate', 'VBP'), ('that', 'IN'), ('hyarm', 'NN'), ('yields', 'NNS'), ('predictable', 'JJ'), (',', ','), ('stable', 'JJ'), (',', ','), ('and', 'CC'), ('high', 'JJ'), ('system', 'NN'), ('performance', 'NN'), (',', ','), ('even', 'RB'), ('in', 'IN'), ('the', 'DT'), ('face', 'NN'), ('of', 'IN'), ('fluctuating', 'VBG'), ('workload', 'NN'), ('and', 'CC'), ('resource', 'NN'), ('availability', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m748syAOS3U",
        "outputId": "61cb9264-c7b8-4381-9f56-4c40ffeb3211",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "adjective_tags = ['JJ','JJR','JJS']\n",
        "\n",
        "lemmatized_text = []\n",
        "\n",
        "for word in pos_tag:\n",
        "    if word[1] in adjective_tags:\n",
        "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
        "    else:\n",
        "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
        "        \n",
        "print (\"Text tokens after lemmatization of adjectives and nouns: \\n\")\n",
        "print (lemmatized_text)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text tokens after lemmatization of adjectives and nouns: \n",
            "\n",
            "['a', 'challenging', 'problem', 'faced', 'by', 'researcher', 'and', 'developer', 'of', 'distributed', 'real-time', 'and', 'embedded', '(', 'dre', ')', 'system', 'is', 'devising', 'and', 'implementing', 'effective', 'adaptive', 'resource', 'management', 'strategy', 'that', 'can', 'meet', 'end-to-end', 'quality', 'of', 'service', '(', 'qos', ')', 'requirement', 'in', 'varying', 'operational', 'condition', '.', 'this', 'paper', 'present', 'two', 'contribution', 'to', 'research', 'in', 'adaptive', 'resource', 'management', 'for', 'dre', 'system', '.', 'first', ',', 'we', 'describe', 'the', 'structure', 'and', 'functionality', 'of', 'the', 'hybrid', 'adaptive', 'resourcemanagement', 'middleware', '(', 'hyarm', ')', ',', 'which', 'provides', 'adaptive', 'resource', 'management', 'using', 'hybrid', 'control', 'technique', 'for', 'adapting', 'to', 'workload', 'fluctuation', 'and', 'resource', 'availability', '.', 'second', ',', 'we', 'evaluate', 'the', 'adaptive', 'behavior', 'of', 'hyarm', 'via', 'experiment', 'on', 'a', 'dre', 'multimedia', 'system', 'that', 'distributes', 'video', 'in', 'real-time', '.', 'our', 'result', 'indicate', 'that', 'hyarm', 'yield', 'predictable', ',', 'stable', ',', 'and', 'high', 'system', 'performance', ',', 'even', 'in', 'the', 'face', 'of', 'fluctuating', 'workload', 'and', 'resource', 'availability']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDUngZFQOS1s",
        "outputId": "14bbb45d-6984-4165-c9ef-a059b26f0c1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pos_tag = nltk.pos_tag(lemmatized_text)\n",
        "\n",
        "print (\"Lemmatized text with POS tags: \\n\")\n",
        "print (pos_tag)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lemmatized text with POS tags: \n",
            "\n",
            "[('a', 'DT'), ('challenging', 'NN'), ('problem', 'NN'), ('faced', 'VBN'), ('by', 'IN'), ('researcher', 'NN'), ('and', 'CC'), ('developer', 'NN'), ('of', 'IN'), ('distributed', 'VBN'), ('real-time', 'NN'), ('and', 'CC'), ('embedded', 'VBD'), ('(', '('), ('dre', 'NN'), (')', ')'), ('system', 'NN'), ('is', 'VBZ'), ('devising', 'VBG'), ('and', 'CC'), ('implementing', 'VBG'), ('effective', 'JJ'), ('adaptive', 'JJ'), ('resource', 'NN'), ('management', 'NN'), ('strategy', 'NN'), ('that', 'WDT'), ('can', 'MD'), ('meet', 'VB'), ('end-to-end', 'JJ'), ('quality', 'NN'), ('of', 'IN'), ('service', 'NN'), ('(', '('), ('qos', 'NN'), (')', ')'), ('requirement', 'NN'), ('in', 'IN'), ('varying', 'VBG'), ('operational', 'JJ'), ('condition', 'NN'), ('.', '.'), ('this', 'DT'), ('paper', 'NN'), ('present', 'JJ'), ('two', 'CD'), ('contribution', 'NN'), ('to', 'TO'), ('research', 'NN'), ('in', 'IN'), ('adaptive', 'JJ'), ('resource', 'NN'), ('management', 'NN'), ('for', 'IN'), ('dre', 'NN'), ('system', 'NN'), ('.', '.'), ('first', 'RB'), (',', ','), ('we', 'PRP'), ('describe', 'VBP'), ('the', 'DT'), ('structure', 'NN'), ('and', 'CC'), ('functionality', 'NN'), ('of', 'IN'), ('the', 'DT'), ('hybrid', 'JJ'), ('adaptive', 'JJ'), ('resourcemanagement', 'NN'), ('middleware', 'NN'), ('(', '('), ('hyarm', 'NN'), (')', ')'), (',', ','), ('which', 'WDT'), ('provides', 'VBZ'), ('adaptive', 'JJ'), ('resource', 'NN'), ('management', 'NN'), ('using', 'VBG'), ('hybrid', 'JJ'), ('control', 'NN'), ('technique', 'NN'), ('for', 'IN'), ('adapting', 'VBG'), ('to', 'TO'), ('workload', 'VB'), ('fluctuation', 'NN'), ('and', 'CC'), ('resource', 'NN'), ('availability', 'NN'), ('.', '.'), ('second', 'JJ'), (',', ','), ('we', 'PRP'), ('evaluate', 'VBP'), ('the', 'DT'), ('adaptive', 'JJ'), ('behavior', 'NN'), ('of', 'IN'), ('hyarm', 'NN'), ('via', 'IN'), ('experiment', 'NN'), ('on', 'IN'), ('a', 'DT'), ('dre', 'NN'), ('multimedia', 'NN'), ('system', 'NN'), ('that', 'WDT'), ('distributes', 'VBZ'), ('video', 'NN'), ('in', 'IN'), ('real-time', 'NN'), ('.', '.'), ('our', 'PRP$'), ('result', 'NN'), ('indicate', 'VBP'), ('that', 'IN'), ('hyarm', 'JJ'), ('yield', 'NN'), ('predictable', 'JJ'), (',', ','), ('stable', 'JJ'), (',', ','), ('and', 'CC'), ('high', 'JJ'), ('system', 'NN'), ('performance', 'NN'), (',', ','), ('even', 'RB'), ('in', 'IN'), ('the', 'DT'), ('face', 'NN'), ('of', 'IN'), ('fluctuating', 'VBG'), ('workload', 'NN'), ('and', 'CC'), ('resource', 'NN'), ('availability', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hggCWsMZPM9e"
      },
      "source": [
        "stopwords = []\n",
        "\n",
        "wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
        "\n",
        "for word in pos_tag:\n",
        "    if word[1] not in wanted_POS:\n",
        "        stopwords.append(word[0])\n",
        "\n",
        "punctuations = list(str(string.punctuation))\n",
        "\n",
        "stopwords = stopwords + punctuations"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEsE-WilPldj",
        "outputId": "8a56f9ce-cf71-4514-d30e-0f02b1300ad2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "processed_text = []\n",
        "for word in lemmatized_text:\n",
        "    if word not in stopwords:\n",
        "        processed_text.append(word)\n",
        "print (processed_text)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['challenging', 'problem', 'researcher', 'developer', 'real-time', 'dre', 'system', 'devising', 'implementing', 'effective', 'adaptive', 'resource', 'management', 'strategy', 'end-to-end', 'quality', 'service', 'qos', 'requirement', 'varying', 'operational', 'condition', 'paper', 'present', 'contribution', 'research', 'adaptive', 'resource', 'management', 'dre', 'system', 'structure', 'functionality', 'hybrid', 'adaptive', 'resourcemanagement', 'middleware', 'hyarm', 'adaptive', 'resource', 'management', 'using', 'hybrid', 'control', 'technique', 'adapting', 'fluctuation', 'resource', 'availability', 'second', 'adaptive', 'behavior', 'hyarm', 'experiment', 'dre', 'multimedia', 'system', 'video', 'real-time', 'result', 'hyarm', 'yield', 'predictable', 'stable', 'high', 'system', 'performance', 'face', 'fluctuating', 'resource', 'availability']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQIeFMccPlgc",
        "outputId": "8527881a-3dc2-4746-bc51-2dc9408fbb90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocabulary = list(set(processed_text))\n",
        "print (vocabulary)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['service', 'face', 'system', 'structure', 'fluctuating', 'resource', 'hyarm', 'strategy', 'operational', 'fluctuation', 'effective', 'technique', 'implementing', 'second', 'varying', 'functionality', 'using', 'devising', 'performance', 'resourcemanagement', 'adaptive', 'stable', 'condition', 'qos', 'research', 'behavior', 'real-time', 'middleware', 'paper', 'researcher', 'developer', 'problem', 'control', 'multimedia', 'requirement', 'result', 'high', 'challenging', 'experiment', 'dre', 'end-to-end', 'contribution', 'availability', 'yield', 'predictable', 'video', 'quality', 'adapting', 'management', 'hybrid', 'present']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_5EovlKSUso",
        "outputId": "6bc7e09e-fae8-4ac9-e889-720d1d44aa53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(vocabulary)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viuHJyFoPli_"
      },
      "source": [
        "vocab_len = len(vocabulary)\n",
        "\n",
        "weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
        "\n",
        "score = np.zeros((vocab_len),dtype=np.float32)\n",
        "window_size = 3\n",
        "covered_coocurrences = []\n",
        "\n",
        "for i in range(0,vocab_len):\n",
        "    score[i]=1\n",
        "    for j in range(0,vocab_len):\n",
        "        if j==i:\n",
        "            weighted_edge[i][j]=0\n",
        "        else:\n",
        "            for window_start in range(0,(len(processed_text)-window_size)):\n",
        "                \n",
        "                window_end = window_start+window_size\n",
        "                \n",
        "                window = processed_text[window_start:window_end]\n",
        "                \n",
        "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
        "                    \n",
        "                    index_of_i = window_start + window.index(vocabulary[i])\n",
        "                    index_of_j = window_start + window.index(vocabulary[j])\n",
        "                    \n",
        "                    # index_of_x is the absolute position of the xth term in the window \n",
        "                    # (counting from 0) \n",
        "                    # in the processed_text\n",
        "                      \n",
        "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
        "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
        "                        covered_coocurrences.append([index_of_i,index_of_j])"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MKqBC63PWV_"
      },
      "source": [
        "inout = np.zeros((vocab_len),dtype=np.float32)\n",
        "\n",
        "for i in range(0,vocab_len):\n",
        "    for j in range(0,vocab_len):\n",
        "        inout[i]+=weighted_edge[i][j]"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bSq1EekPWZN",
        "outputId": "bdfaafd3-9c24-416b-b617-a5b2ecb23b5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "MAX_ITERATIONS = 50\n",
        "d=0.85\n",
        "threshold = 0.0001 #convergence threshold\n",
        "\n",
        "for iter in range(0,MAX_ITERATIONS):\n",
        "    prev_score = np.copy(score)\n",
        "    \n",
        "    for i in range(0,vocab_len):\n",
        "        \n",
        "        summation = 0\n",
        "        for j in range(0,vocab_len):\n",
        "            if weighted_edge[i][j] != 0:\n",
        "                summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
        "                \n",
        "        score[i] = (1-d) + d*(summation)\n",
        "    \n",
        "    if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n",
        "        print(\"Converging at iteration \"+str(iter)+\"....\")\n",
        "        break"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converging at iteration 29....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfDhemVOPWS1",
        "outputId": "0337c54b-740f-4546-bc37-24688cb1edba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(0,vocab_len):\n",
        "    print(\"Score of \"+vocabulary[i]+\": \"+str(score[i]))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score of service: 0.92646396\n",
            "Score of face: 0.76746017\n",
            "Score of system: 2.5743918\n",
            "Score of structure: 0.72268647\n",
            "Score of fluctuating: 0.6502049\n",
            "Score of resource: 2.7561345\n",
            "Score of hyarm: 1.9326094\n",
            "Score of strategy: 0.7752871\n",
            "Score of operational: 0.9571497\n",
            "Score of fluctuation: 0.7626338\n",
            "Score of effective: 0.7236684\n",
            "Score of technique: 0.79915214\n",
            "Score of implementing: 0.74068487\n",
            "Score of second: 0.71423787\n",
            "Score of varying: 0.96057063\n",
            "Score of functionality: 0.7243751\n",
            "Score of using: 0.71277857\n",
            "Score of devising: 0.73387665\n",
            "Score of performance: 0.770985\n",
            "Score of resourcemanagement: 0.7105836\n",
            "Score of adaptive: 3.0542428\n",
            "Score of stable: 0.7988939\n",
            "Score of condition: 0.94644433\n",
            "Score of qos: 0.9465402\n",
            "Score of research: 0.77476484\n",
            "Score of behavior: 0.7077215\n",
            "Score of real-time: 1.3990164\n",
            "Score of middleware: 0.70693004\n",
            "Score of paper: 0.92629725\n",
            "Score of researcher: 0.95136374\n",
            "Score of developer: 0.854713\n",
            "Score of problem: 0.869543\n",
            "Score of control: 0.781213\n",
            "Score of multimedia: 0.7160706\n",
            "Score of requirement: 0.9571934\n",
            "Score of result: 0.74536335\n",
            "Score of high: 0.78106034\n",
            "Score of challenging: 0.58042115\n",
            "Score of experiment: 0.71251506\n",
            "Score of dre: 1.8877407\n",
            "Score of end-to-end: 0.84114546\n",
            "Score of contribution: 0.84071887\n",
            "Score of availability: 0.7204785\n",
            "Score of yield: 0.7775568\n",
            "Score of predictable: 0.7985734\n",
            "Score of video: 0.7375832\n",
            "Score of quality: 0.8947482\n",
            "Score of adapting: 0.78994507\n",
            "Score of management: 1.836408\n",
            "Score of hybrid: 1.3545902\n",
            "Score of present: 0.89449257\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRKSmdPQPNEa",
        "outputId": "ce816ebe-92b9-44c1-8e09-6b36d02b595c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "phrases = []\n",
        "\n",
        "phrase = \" \"\n",
        "for word in lemmatized_text:\n",
        "    \n",
        "    if word in stopwords:\n",
        "        if phrase!= \" \":\n",
        "            phrases.append(str(phrase).strip().split())\n",
        "        phrase = \" \"\n",
        "    elif word not in stopwords:\n",
        "        phrase+=str(word)\n",
        "        phrase+=\" \"\n",
        "\n",
        "print(\"Partitioned Phrases (Candidate Keyphrases): \\n\")\n",
        "print(phrases)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Partitioned Phrases (Candidate Keyphrases): \n",
            "\n",
            "[['challenging', 'problem'], ['researcher'], ['developer'], ['real-time'], ['dre'], ['system'], ['devising'], ['implementing', 'effective', 'adaptive', 'resource', 'management', 'strategy'], ['end-to-end', 'quality'], ['service'], ['qos'], ['requirement'], ['varying', 'operational', 'condition'], ['paper', 'present'], ['contribution'], ['research'], ['adaptive', 'resource', 'management'], ['dre', 'system'], ['structure'], ['functionality'], ['hybrid', 'adaptive', 'resourcemanagement', 'middleware'], ['hyarm'], ['adaptive', 'resource', 'management', 'using', 'hybrid', 'control', 'technique'], ['adapting'], ['fluctuation'], ['resource', 'availability'], ['second'], ['adaptive', 'behavior'], ['hyarm'], ['experiment'], ['dre', 'multimedia', 'system'], ['video'], ['real-time'], ['result'], ['hyarm', 'yield', 'predictable'], ['stable'], ['high', 'system', 'performance'], ['face'], ['fluctuating']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jg9XxhfPM42",
        "outputId": "378b6ca0-8d3e-48ff-f751-c3e38777f4e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "unique_phrases = []\n",
        "\n",
        "for phrase in phrases:\n",
        "    if phrase not in unique_phrases:\n",
        "        unique_phrases.append(phrase)\n",
        "\n",
        "print(\"Unique Phrases (Candidate Keyphrases): \\n\")\n",
        "print(unique_phrases)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique Phrases (Candidate Keyphrases): \n",
            "\n",
            "[['challenging', 'problem'], ['researcher'], ['developer'], ['real-time'], ['dre'], ['system'], ['devising'], ['implementing', 'effective', 'adaptive', 'resource', 'management', 'strategy'], ['end-to-end', 'quality'], ['service'], ['qos'], ['requirement'], ['varying', 'operational', 'condition'], ['paper', 'present'], ['contribution'], ['research'], ['adaptive', 'resource', 'management'], ['dre', 'system'], ['structure'], ['functionality'], ['hybrid', 'adaptive', 'resourcemanagement', 'middleware'], ['hyarm'], ['adaptive', 'resource', 'management', 'using', 'hybrid', 'control', 'technique'], ['adapting'], ['fluctuation'], ['resource', 'availability'], ['second'], ['adaptive', 'behavior'], ['experiment'], ['dre', 'multimedia', 'system'], ['video'], ['result'], ['hyarm', 'yield', 'predictable'], ['stable'], ['high', 'system', 'performance'], ['face'], ['fluctuating']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3w_yRgWOSzq",
        "outputId": "afa8c1d7-e5cd-4afd-e7ff-be85d8ff9b7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "for word in vocabulary:\n",
        "    #print word\n",
        "    for phrase in unique_phrases:\n",
        "        if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n",
        "            #if len(phrase)>1 then the current phrase is multi-worded.\n",
        "            #if the word in vocabulary is present in unique_phrases as a single-word-phrase\n",
        "            # and at the same time present as a word within a multi-worded phrase,\n",
        "            # then I will remove the single-word-phrase from the list.\n",
        "            unique_phrases.remove([word])\n",
        "            \n",
        "print(\"Thinned Unique Phrases (Candidate Keyphrases): \\n\")\n",
        "print(unique_phrases)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thinned Unique Phrases (Candidate Keyphrases): \n",
            "\n",
            "[['challenging', 'problem'], ['researcher'], ['developer'], ['real-time'], ['devising'], ['implementing', 'effective', 'adaptive', 'resource', 'management', 'strategy'], ['end-to-end', 'quality'], ['service'], ['qos'], ['requirement'], ['varying', 'operational', 'condition'], ['paper', 'present'], ['contribution'], ['research'], ['adaptive', 'resource', 'management'], ['dre', 'system'], ['structure'], ['functionality'], ['hybrid', 'adaptive', 'resourcemanagement', 'middleware'], ['adaptive', 'resource', 'management', 'using', 'hybrid', 'control', 'technique'], ['adapting'], ['fluctuation'], ['resource', 'availability'], ['second'], ['adaptive', 'behavior'], ['experiment'], ['dre', 'multimedia', 'system'], ['video'], ['result'], ['hyarm', 'yield', 'predictable'], ['stable'], ['high', 'system', 'performance'], ['face'], ['fluctuating']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM3SQceCOSyA"
      },
      "source": [
        "phrase_scores = []\n",
        "keywords = []\n",
        "for phrase in unique_phrases:\n",
        "    phrase_score=0\n",
        "    keyword = ''\n",
        "    for word in phrase:\n",
        "        keyword += str(word)\n",
        "        keyword += \" \"\n",
        "        phrase_score+=score[vocabulary.index(word)]\n",
        "    phrase_scores.append(phrase_score)\n",
        "    keywords.append(keyword.strip())\n",
        "\n",
        "res = {keywords[i]: phrase_scores[i] for i in range(len(keywords))} \n",
        "\n",
        "# i=0\n",
        "# for keyword in keywords:\n",
        "#     print (\"Keyword: '\"+str(keyword)+\"', Score: \"+str(phrase_scores[i]))\n",
        "#     i+=1"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL25_wy3OSre",
        "outputId": "65177f44-62ed-4eeb-f07a-d83227efda6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "res_sorted_keys = sorted(res, key=res.get, reverse=True)\n",
        "for r in res_sorted_keys[0:5]:\n",
        "    print(r, res[r])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adaptive resource management using hybrid control technique 11.294519245624542\n",
            "implementing effective adaptive resource management strategy 9.886425733566284\n",
            "adaptive resource management 7.646785378456116\n",
            "hybrid adaptive resourcemanagement middleware 5.826346695423126\n",
            "dre multimedia system 5.1782031655311584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmrMXUdqBkto",
        "outputId": "09068788-615f-48a4-c02a-cf4c353464b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
        "\n",
        "keywords_num = 10\n",
        "\n",
        "print(\"Keywords:\\n\")\n",
        "\n",
        "for i in range(0,keywords_num):\n",
        "    print(str(keywords[sorted_index[i]])+\", \", end=' ')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keywords:\n",
            "\n",
            "adaptive resource management using hybrid control technique,  implementing effective adaptive resource management strategy,  adaptive resource management,  hybrid adaptive resourcemanagement middleware,  dre multimedia system,  dre system,  high system performance,  adaptive behavior,  hyarm yield predictable,  resource availability,  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6DfbPuhXK12",
        "outputId": "7b9d5d57-0b3d-48d8-df7c-d1df3e1e3d7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "keywords"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['challenging problem',\n",
              " 'researcher',\n",
              " 'developer',\n",
              " 'real-time',\n",
              " 'devising',\n",
              " 'implementing effective adaptive resource management strategy',\n",
              " 'end-to-end quality',\n",
              " 'service',\n",
              " 'qos',\n",
              " 'requirement',\n",
              " 'varying operational condition',\n",
              " 'paper present',\n",
              " 'contribution',\n",
              " 'research',\n",
              " 'adaptive resource management',\n",
              " 'dre system',\n",
              " 'structure',\n",
              " 'functionality',\n",
              " 'hybrid adaptive resourcemanagement middleware',\n",
              " 'adaptive resource management using hybrid control technique',\n",
              " 'adapting',\n",
              " 'fluctuation',\n",
              " 'resource availability',\n",
              " 'second',\n",
              " 'adaptive behavior',\n",
              " 'experiment',\n",
              " 'dre multimedia system',\n",
              " 'video',\n",
              " 'result',\n",
              " 'hyarm yield predictable',\n",
              " 'stable',\n",
              " 'high system performance',\n",
              " 'face',\n",
              " 'fluctuating']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLuicAqwKk97"
      },
      "source": [
        "### Ground Truth\n",
        "\n",
        "adaptive resource management,\n",
        "distributed real-time embedded system,\n",
        "end-to-end quality of service,\n",
        "service end-to-end quality,\n",
        "hybrid adaptive resourcemanagement middleware,\n",
        "hybrid control technique,\n",
        "real-time video distribution system,\n",
        "real-time corba specification,\n",
        "video encoding/decoding,\n",
        "resource reservation mechanism,\n",
        "dynamic environment,\n",
        "streaming service,\n",
        "distribute real-time embed system,\n",
        "hybrid system,\n",
        "quality of service,\n",
        "service quality\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUp29yGOKyI9"
      },
      "source": [
        "ground_truth = ['adaptive resource management', 'distributed real-time embedded system', 'end-to-end quality of service', 'service end-to-end quality', 'hybrid adaptive resourcemanagement middleware', 'hybrid control technique', 'real-time video distribution system', 'real-time corba specification', 'video encoding/decoding', 'resource reservation mechanism', 'dynamic environment', 'streaming service', 'distribute real-time embed system', 'hybrid system', 'quality of service','service quality']"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1chsGXQZBq9",
        "outputId": "86f75700-4af8-4770-ade6-ac52ffa73825",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(ground_truth)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrWWfAO7YyOQ",
        "outputId": "9c4e3db4-c109-4c15-b838-a15934a6c458",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(set(keywords) & (set(ground_truth)))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adaptive resource management',\n",
              " 'hybrid adaptive resourcemanagement middleware']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeszOumBaVNL"
      },
      "source": [
        "### Applying the above on KP_20k dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlMQ7SnnY79E"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Advanced NLP-Project/data5L.csv\")"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xeqp-woFkR8o",
        "outputId": "babc16f2-eec4-4540-c1b4-575bac1c7e7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>keyword</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This paper proposes using virtual reality to e...</td>\n",
              "      <td>telepresence;animation;avatars;application sha...</td>\n",
              "      <td>virtually enhancing the perception of user act...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This paper presents an improved architecture o...</td>\n",
              "      <td>sigma delta modulators;analog-to-digital conve...</td>\n",
              "      <td>Dynamic range improvement of multistage multib...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In this paper, we discuss the motivation and t...</td>\n",
              "      <td>enterprise information integration and interop...</td>\n",
              "      <td>An ontology modelling perspective on business ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>An overview of the self-organizing map algorit...</td>\n",
              "      <td>self-organizing map;learning vector quantization</td>\n",
              "      <td>The self-organizing map</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The amygdala comprises part of an extended net...</td>\n",
              "      <td>social brain;amygdala;behavior;facial expression</td>\n",
              "      <td>The Amygdala and Development of the Social Brain</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            abstract  ...                                              title\n",
              "0  This paper proposes using virtual reality to e...  ...  virtually enhancing the perception of user act...\n",
              "1  This paper presents an improved architecture o...  ...  Dynamic range improvement of multistage multib...\n",
              "2  In this paper, we discuss the motivation and t...  ...  An ontology modelling perspective on business ...\n",
              "3  An overview of the self-organizing map algorit...  ...                            The self-organizing map\n",
              "4  The amygdala comprises part of an extended net...  ...   The Amygdala and Development of the Social Brain\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv6pulGJkJg3"
      },
      "source": [
        "data['abs_keyword_count'] = data.keyword.str.strip().str.split(';').apply(len)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo6PrKb6kbNn",
        "outputId": "cf5509b3-9fd0-4880-f351-aa728320015b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.sample(5)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>keyword</th>\n",
              "      <th>title</th>\n",
              "      <th>abs_keyword_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>78867</th>\n",
              "      <td>Forage quality in grassland-savanna ecosystems...</td>\n",
              "      <td>landscape;modelling;monitoring;ecology;resourc...</td>\n",
              "      <td>Remote sensing of forage nutrients: Combining ...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319784</th>\n",
              "      <td>Application of network coding in wireless two-...</td>\n",
              "      <td>channel coding;ergodic capacity;log-likelihood...</td>\n",
              "      <td>Soft Network Coding in Wireless Two-Way Relay ...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437631</th>\n",
              "      <td>The increasing pervasiveness of location-acqui...</td>\n",
              "      <td>network;pervasive;applications;sequential patt...</td>\n",
              "      <td>trajectory pattern mining</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278568</th>\n",
              "      <td>Load balancing middleware is used extensively ...</td>\n",
              "      <td>middleware;patterns;scalability;corba;load bal...</td>\n",
              "      <td>Issues in the design of adaptive middleware lo...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170061</th>\n",
              "      <td>The concept of Just-In-Time Minus (JIT(-)) cus...</td>\n",
              "      <td>e-commerce;customer service;information system...</td>\n",
              "      <td>An Intranet based information system supportin...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 abstract  ... abs_keyword_count\n",
              "78867   Forage quality in grassland-savanna ecosystems...  ...                 6\n",
              "319784  Application of network coding in wireless two-...  ...                 7\n",
              "437631  The increasing pervasiveness of location-acqui...  ...                29\n",
              "278568  Load balancing middleware is used extensively ...  ...                 5\n",
              "170061  The concept of Just-In-Time Minus (JIT(-)) cus...  ...                 5\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8rVJLXpkKC4",
        "outputId": "c74f7031-694f-4300-c3ac-f07482d80a06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "max(data['abs_keyword_count']), min(data['abs_keyword_count'])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(110, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoRSpzN4ajmn"
      },
      "source": [
        "def clean(text):\n",
        "    text = text.lower()\n",
        "    printable = set(string.printable)\n",
        "    text = filter(lambda x: x in printable, text)\n",
        "    text = \"\".join(list(text))\n",
        "    return text\n",
        "\n",
        "def TextScoring(text):\n",
        "  cleaned_text = clean(text)\n",
        "  text = word_tokenize(cleaned_text)\n",
        "  pos_tag = nltk.pos_tag(text)\n",
        "  wordnet_lemmatizer = WordNetLemmatizer()\n",
        "  adjective_tags = ['JJ','JJR','JJS']\n",
        "  lemmatized_text = []\n",
        "  for word in pos_tag:\n",
        "      if word[1] in adjective_tags:\n",
        "          lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
        "      else:\n",
        "          lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0])))\n",
        "  pos_tag = nltk.pos_tag(lemmatized_text)\n",
        "  stopwords = []\n",
        "  wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS'] \n",
        "  for word in pos_tag:\n",
        "      if word[1] not in wanted_POS:\n",
        "          stopwords.append(word[0])\n",
        "  punctuations = list(str(string.punctuation))\n",
        "  stopwords = stopwords + punctuations\n",
        "  processed_text = []\n",
        "  for word in lemmatized_text:\n",
        "      if word not in stopwords:\n",
        "          processed_text.append(word)\n",
        "  vocabulary = list(set(processed_text))\n",
        "  vocab_len = len(vocabulary)\n",
        "\n",
        "  weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
        "\n",
        "  score = np.zeros((vocab_len),dtype=np.float32)\n",
        "  window_size = 3\n",
        "  covered_coocurrences = []\n",
        "\n",
        "  for i in range(0,vocab_len):\n",
        "      score[i]=1\n",
        "      for j in range(0,vocab_len):\n",
        "          if j==i:\n",
        "              weighted_edge[i][j]=0\n",
        "          else:\n",
        "              for window_start in range(0,(len(processed_text)-window_size)):\n",
        "                  \n",
        "                  window_end = window_start+window_size\n",
        "                  \n",
        "                  window = processed_text[window_start:window_end]\n",
        "                  \n",
        "                  if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
        "                      \n",
        "                      index_of_i = window_start + window.index(vocabulary[i])\n",
        "                      index_of_j = window_start + window.index(vocabulary[j])\n",
        "                      \n",
        "                      # index_of_x is the absolute position of the xth term in the window \n",
        "                      # (counting from 0) \n",
        "                      # in the processed_text\n",
        "                        \n",
        "                      if [index_of_i,index_of_j] not in covered_coocurrences:\n",
        "                          weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
        "                          covered_coocurrences.append([index_of_i,index_of_j])\n",
        "\n",
        "  inout = np.zeros((vocab_len),dtype=np.float32)\n",
        "\n",
        "  for i in range(0,vocab_len):\n",
        "      for j in range(0,vocab_len):\n",
        "          inout[i]+=weighted_edge[i][j]\n",
        "\n",
        "  MAX_ITERATIONS = 50\n",
        "  d=0.85\n",
        "  threshold = 0.0001 #convergence threshold\n",
        "\n",
        "  for iter in range(0,MAX_ITERATIONS):\n",
        "      prev_score = np.copy(score)\n",
        "      \n",
        "      for i in range(0,vocab_len):\n",
        "          \n",
        "          summation = 0\n",
        "          for j in range(0,vocab_len):\n",
        "              if weighted_edge[i][j] != 0:\n",
        "                  summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
        "                  \n",
        "          score[i] = (1-d) + d*(summation)\n",
        "      \n",
        "      if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n",
        "          #print(\"Converging at iteration \"+str(iter)+\"....\")\n",
        "          break\n",
        "  phrases = []\n",
        "\n",
        "  phrase = \" \"\n",
        "  for word in lemmatized_text:\n",
        "      \n",
        "      if word in stopwords:\n",
        "          if phrase!= \" \":\n",
        "              phrases.append(str(phrase).strip().split())\n",
        "          phrase = \" \"\n",
        "      elif word not in stopwords:\n",
        "          phrase+=str(word)\n",
        "          phrase+=\" \"\n",
        "\n",
        "  unique_phrases = []\n",
        "  for phrase in phrases:\n",
        "      if phrase not in unique_phrases:\n",
        "          unique_phrases.append(phrase)\n",
        "\n",
        "  for word in vocabulary:\n",
        "      #print word\n",
        "      for phrase in unique_phrases:\n",
        "          if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n",
        "              unique_phrases.remove([word])\n",
        "\n",
        "  phrase_scores = []\n",
        "  keywords = []\n",
        "  for phrase in unique_phrases:\n",
        "      phrase_score=0\n",
        "      keyword = ''\n",
        "      for word in phrase:\n",
        "          keyword += str(word)\n",
        "          keyword += \" \"\n",
        "          phrase_score+=score[vocabulary.index(word)]\n",
        "      phrase_scores.append(phrase_score)\n",
        "      keywords.append(keyword.strip())\n",
        "\n",
        "  res = {keywords[i]: phrase_scores[i] for i in range(len(keywords))}\n",
        "  sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
        "  keywords_num = len(keywords)\n",
        "  final_keywords = []\n",
        "  for i in range(0,keywords_num):\n",
        "    final_keywords.append(str(keywords[sorted_index[i]]))\n",
        "  return final_keywords\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voFQQFmVym4c"
      },
      "source": [
        "data_5 = data[0:5]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhZcaZlNytwV"
      },
      "source": [
        "data_5['TextScoring'] = data_5['abstract'].apply(lambda x: \",\".join(TextScoring(x)))"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL2AjvSuyw4m",
        "outputId": "eb29c73e-e1fa-480e-a7ce-382984a48178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "data_5"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>keyword</th>\n",
              "      <th>title</th>\n",
              "      <th>abs_keyword_count</th>\n",
              "      <th>TextScoring</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This paper proposes using virtual reality to e...</td>\n",
              "      <td>telepresence;animation;avatars;application sha...</td>\n",
              "      <td>virtually enhancing the perception of user act...</td>\n",
              "      <td>5</td>\n",
              "      <td>user action,recorded action,remote synchronous...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This paper presents an improved architecture o...</td>\n",
              "      <td>sigma delta modulators;analog-to-digital conve...</td>\n",
              "      <td>Dynamic range improvement of multistage multib...</td>\n",
              "      <td>5</td>\n",
              "      <td>leakage quantization noise problem,in-band qua...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In this paper, we discuss the motivation and t...</td>\n",
              "      <td>enterprise information integration and interop...</td>\n",
              "      <td>An ontology modelling perspective on business ...</td>\n",
              "      <td>5</td>\n",
              "      <td>business reporting language structure,extensib...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>An overview of the self-organizing map algorit...</td>\n",
              "      <td>self-organizing map;learning vector quantization</td>\n",
              "      <td>The self-organizing map</td>\n",
              "      <td>2</td>\n",
              "      <td>self-organizing map algorithm,paper,issue,over...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The amygdala comprises part of an extended net...</td>\n",
              "      <td>social brain;amygdala;behavior;facial expression</td>\n",
              "      <td>The Amygdala and Development of the Social Brain</td>\n",
              "      <td>4</td>\n",
              "      <td>social cognitive network,social behavior appro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            abstract  ...                                        TextScoring\n",
              "0  This paper proposes using virtual reality to e...  ...  user action,recorded action,remote synchronous...\n",
              "1  This paper presents an improved architecture o...  ...  leakage quantization noise problem,in-band qua...\n",
              "2  In this paper, we discuss the motivation and t...  ...  business reporting language structure,extensib...\n",
              "3  An overview of the self-organizing map algorit...  ...  self-organizing map algorithm,paper,issue,over...\n",
              "4  The amygdala comprises part of an extended net...  ...  social cognitive network,social behavior appro...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5mTWFx_q1Cn"
      },
      "source": [
        "data_1000 = data[0:1000]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKgPxCVVqqwS",
        "outputId": "f07beb1b-25c3-45d0-8a27-7b544b603866",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "data_1000['TextScoring'] = data_1000['abstract'].apply(lambda x: \",\".join(TextScoring(x)))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4min 2s, sys: 85.2 ms, total: 4min 2s\n",
            "Wall time: 4min 3s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE6nB2zpH8G3",
        "outputId": "70484846-0154-43fb-83b6-148ccc3c63c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_1000.TextScoring.sample(5)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "253    labelled petri net,equivalence land preorders,...\n",
              "876    such power aware system,power aware system,pow...\n",
              "475    new delay-dependent stability criterion,uncert...\n",
              "885    computer science privacy research,privacy solu...\n",
              "86     adjustable regulated power output,input curren...\n",
              "Name: TextScoring, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcVQADdHajoz",
        "outputId": "c806ffc9-a455-433f-cc62-101d81c4e28b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "#data['TextScoring'] = data['abstract'].apply(lambda x: \",\".join(TextScoring(x)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 5.72 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2cZXOidaj3V",
        "outputId": "662de79e-6983-4c41-9a33-ec8a9d4319f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "data_1000.sample(5)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>keyword</th>\n",
              "      <th>title</th>\n",
              "      <th>abs_keyword_count</th>\n",
              "      <th>TextScoring</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>In this paper, a new grid generation system is...</td>\n",
              "      <td>unstructured surface mesh generation;geometry ...</td>\n",
              "      <td>Parallel generation of unstructured surface grids</td>\n",
              "      <td>5</td>\n",
              "      <td>new grid generation system,unstructured triang...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>Edit distance based string similarity join is ...</td>\n",
              "      <td>string joins;probabilistic strings;approximate...</td>\n",
              "      <td>probabilistic string similarity joins</td>\n",
              "      <td>3</td>\n",
              "      <td>probabilistic data,relational database engine,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>957</th>\n",
              "      <td>The mass transfer between immiscible two-liqui...</td>\n",
              "      <td>bubble;immiscible two-liquid interface;ripple;...</td>\n",
              "      <td>Micro droplets generated on a rising bubble th...</td>\n",
              "      <td>5</td>\n",
              "      <td>numerous micro water droplet,fine water drople...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>This paper reports the initial results of a re...</td>\n",
              "      <td>artifact mediated collaboration;hci design pat...</td>\n",
              "      <td>user requirements for a web based spreadsheet-...</td>\n",
              "      <td>4</td>\n",
              "      <td>cooperative interaction design,scenario-based ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>780</th>\n",
              "      <td>Achieving ultra-large-scale software systems w...</td>\n",
              "      <td>service;software engineering;development proce...</td>\n",
              "      <td>a service driven development process (sddp) mo...</td>\n",
              "      <td>4</td>\n",
              "      <td>proposed process model,special development pro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              abstract  ...                                        TextScoring\n",
              "28   In this paper, a new grid generation system is...  ...  new grid generation system,unstructured triang...\n",
              "188  Edit distance based string similarity join is ...  ...  probabilistic data,relational database engine,...\n",
              "957  The mass transfer between immiscible two-liqui...  ...  numerous micro water droplet,fine water drople...\n",
              "451  This paper reports the initial results of a re...  ...  cooperative interaction design,scenario-based ...\n",
              "780  Achieving ultra-large-scale software systems w...  ...  proposed process model,special development pro...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbShRn-yca2W"
      },
      "source": [
        "data_1000['keyword'] = data_1000['keyword'].str.replace(';',',')"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bNVfjdGi8QC"
      },
      "source": [
        "lst = data_1000.keyword.apply(lambda x: x.strip(',').split(','))"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_wBaLk9iofY"
      },
      "source": [
        "data_1000['keyword'] = data_1000.keyword.apply(lambda x: x.strip(',').split(','))\n",
        "data_1000['TextScoring'] = data_1000.TextScoring.apply(lambda x: x.strip(',').split(','))"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlEf5giCqETz"
      },
      "source": [
        "def normalize_answer(s):\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "    return ' '.join([lower(x) for x in s]).rstrip()"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K8-Cv4GpvcU"
      },
      "source": [
        "def remove_empty(a_list):\n",
        "    new_list = []\n",
        "    for i in a_list:\n",
        "        if len(i) > 0:\n",
        "            if len(i[0]) >0:\n",
        "                new_list.append(normalize_answer(i))   \n",
        "    return new_list"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y-2OJmwcXxr"
      },
      "source": [
        "def dedup(kp_list):\n",
        "    dedupset = set()\n",
        "    kp_list_dedup = []\n",
        "    for kp in kp_list:\n",
        "        if kp in dedupset:\n",
        "            continue       \n",
        "        kp_list_dedup.append(kp)\n",
        "        dedupset.add(kp)\n",
        "    return kp_list_dedup"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbLvml_AjvJb",
        "outputId": "5881c102-6ae1-414a-8c67-63a2b8352f78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dedup(lst[0])"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['telepresence',\n",
              " 'animation',\n",
              " 'avatars',\n",
              " 'application sharing',\n",
              " 'collaborative virtual environments']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7GcRyy8cT-M"
      },
      "source": [
        "def get_score_full(candidates, references, maxDepth = 30):\n",
        "    precision = []\n",
        "    recall = []\n",
        "    reference_set = set(dedup(references))\n",
        "    candidates = dedup(candidates)\n",
        "    referencelen = len(reference_set)\n",
        "    true_positive = 0\n",
        "    for i in range(maxDepth):\n",
        "        if len(candidates) > i:\n",
        "            kp_pred = candidates[i]     \n",
        "            if kp_pred in reference_set:\n",
        "                true_positive += 1\n",
        "        precision.append(true_positive/float(i + 1))\n",
        "        recall.append(true_positive/float(referencelen))\n",
        "    return precision, recall"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_abJWsiyQym"
      },
      "source": [
        "def evaluate(candidates, references,data):\n",
        "    precision_scores, recall_scores, f1_scores = {1:[], 3:[], 5:[], 10:[], 30:[]}, {1:[], 3:[], 5:[], 10:[], 30:[]}, {1:[], 3:[], 5:[], 10:[], 30:[]}\n",
        "    for url in range(len(data)):\n",
        "      candidate = remove_empty(candidates[url])\n",
        "      reference = remove_empty(references[url])\n",
        "    p, r = get_score_full(candidate, reference) \n",
        "    for i in [1,3,5,10,30]:\n",
        "        precision = p[i-1]\n",
        "        recall = r[i-1]\n",
        "        if precision + recall > 0:\n",
        "            f1_scores[i].append((2 * (precision * recall)) / (precision + recall))\n",
        "        else:\n",
        "            f1_scores[i].append(0)\n",
        "        precision_scores[i].append(precision)\n",
        "        recall_scores[i].append(recall)\n",
        "    print(\"########################\\nMetrics\")\n",
        "    for i in precision_scores:\n",
        "        print(\"@{}\".format(i))\n",
        "        print(\"F1:{}\".format(np.mean(f1_scores[i])))\n",
        "        print(\"P:{}\".format(np.mean(precision_scores[i])))\n",
        "        print(\"R:{}\".format(np.mean(recall_scores[i])))\n",
        "    print(\"#########################\")"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1SZEI5mqOLM",
        "outputId": "3ca1caab-1953-41e5-e560-672d4ee0eb3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluate(data_1000['keyword'],data_1000['TextScoring'],data_1000)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "########################\n",
            "Metrics\n",
            "@1\n",
            "F1:0.0\n",
            "P:0.0\n",
            "R:0.0\n",
            "@3\n",
            "F1:0.1111111111111111\n",
            "P:0.6666666666666666\n",
            "R:0.06060606060606061\n",
            "@5\n",
            "F1:0.15789473684210525\n",
            "P:0.6\n",
            "R:0.09090909090909091\n",
            "@10\n",
            "F1:0.13953488372093023\n",
            "P:0.3\n",
            "R:0.09090909090909091\n",
            "@30\n",
            "F1:0.09523809523809525\n",
            "P:0.1\n",
            "R:0.09090909090909091\n",
            "#########################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e23HPUcSyQvG",
        "outputId": "a06ae538-23d0-4b2b-e826-cd0c9ae9be8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluate(data_1000['keyword'],data_1000['TextScoring'],data_1000)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "########################\n",
            "Metrics\n",
            "@1\n",
            "F1:0.0\n",
            "P:0.0\n",
            "R:0.0\n",
            "@3\n",
            "F1:0.1111111111111111\n",
            "P:0.6666666666666666\n",
            "R:0.06060606060606061\n",
            "@5\n",
            "F1:0.15789473684210525\n",
            "P:0.6\n",
            "R:0.09090909090909091\n",
            "@10\n",
            "F1:0.13953488372093023\n",
            "P:0.3\n",
            "R:0.09090909090909091\n",
            "@30\n",
            "F1:0.09523809523809525\n",
            "P:0.1\n",
            "R:0.09090909090909091\n",
            "#########################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By7T6Q5xTQgt",
        "outputId": "cda6cd42-d9f7-4478-f38a-b21391a022b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip '/content/drive/My Drive/Colab Notebooks/Advanced NLP-Project/Project/kp20k_new.zip'"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Colab Notebooks/Advanced NLP-Project/Project/kp20k_new.zip\n",
            "  inflating: kp20k_testing.json      \n",
            "  inflating: kp20k_training.json     \n",
            "  inflating: kp20k_validation.json   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UakRQ0gMUZsR"
      },
      "source": [
        "import json \n",
        "test = []\n",
        "for line in open('/content/kp20k_testing.json', 'r'):\n",
        "    test.append(json.loads(line))"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA6rPn1qs-R2"
      },
      "source": [
        "import json \n",
        "validation = []\n",
        "for line in open('/content/kp20k_validation.json', 'r'):\n",
        "    validation.append(json.loads(line))"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV9fu-3ZU8Y4"
      },
      "source": [
        "test_data = pd.DataFrame(test)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t6VE4d1tFHX"
      },
      "source": [
        "val_data = pd.DataFrame(validation)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDbP9B93VLvG",
        "outputId": "7c4f8eb9-9bb0-4b27-ae8a-5bc350eb1800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "test_data"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>keyword</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A feedback vertex set of a graph G is a set S ...</td>\n",
              "      <td>feedback vertex set;decycling set;2-degenerate...</td>\n",
              "      <td>A feedback vertex set of 2-degenerate graphs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This article proposes techniques to predict th...</td>\n",
              "      <td>performance;analytical modeling;pending hit;da...</td>\n",
              "      <td>Hybrid Analytical Modeling of Pending Cache Hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Autoimmune polyendocrinopathy candidiasis ecto...</td>\n",
              "      <td>apeced;aire;chronic mucocutaneous candidiasis;...</td>\n",
              "      <td>Autoimmune polyendocrinopathy candidiasis ecto...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>In this paper, we consider an enthalpy formula...</td>\n",
              "      <td>casting;thermal;conduction;convection;finite e...</td>\n",
              "      <td>Numerical solution of a three-dimensional soli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In this research, a new type of manufacturing ...</td>\n",
              "      <td>feature recognition;rib;aircraft structural pa...</td>\n",
              "      <td>Definition and recognition of rib features in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>Energy efficiency and transmission delay are v...</td>\n",
              "      <td>energy efficiency;delay;unreliable links;wirel...</td>\n",
              "      <td>Energy-delay tradeoff in wireless multihop net...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>This paper describes the design and implementa...</td>\n",
              "      <td>e-medical records;e-health;e-clinic;web-based;...</td>\n",
              "      <td>A Cyber Medical Center</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>This work describes a detailed simulation-base...</td>\n",
              "      <td>wireless lan;quality of service;medium access ...</td>\n",
              "      <td>adapting wlan mac parameters to enhance voip c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>This paper describes a conceptually simple but...</td>\n",
              "      <td>interior point methods;ellipsoid method;multio...</td>\n",
              "      <td>An interior point multiobjective programming a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>Most of the reports on the method of fundament...</td>\n",
              "      <td>method of fundamental solutions;method of part...</td>\n",
              "      <td>The method of fundamental solutions and its co...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                abstract  ...                                              title\n",
              "0      A feedback vertex set of a graph G is a set S ...  ...       A feedback vertex set of 2-degenerate graphs\n",
              "1      This article proposes techniques to predict th...  ...  Hybrid Analytical Modeling of Pending Cache Hi...\n",
              "2      Autoimmune polyendocrinopathy candidiasis ecto...  ...  Autoimmune polyendocrinopathy candidiasis ecto...\n",
              "3      In this paper, we consider an enthalpy formula...  ...  Numerical solution of a three-dimensional soli...\n",
              "4      In this research, a new type of manufacturing ...  ...  Definition and recognition of rib features in ...\n",
              "...                                                  ...  ...                                                ...\n",
              "19995  Energy efficiency and transmission delay are v...  ...  Energy-delay tradeoff in wireless multihop net...\n",
              "19996  This paper describes the design and implementa...  ...                             A Cyber Medical Center\n",
              "19997  This work describes a detailed simulation-base...  ...  adapting wlan mac parameters to enhance voip c...\n",
              "19998  This paper describes a conceptually simple but...  ...  An interior point multiobjective programming a...\n",
              "19999  Most of the reports on the method of fundament...  ...  The method of fundamental solutions and its co...\n",
              "\n",
              "[20000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zx-i3M2t8T9",
        "outputId": "5986add0-dc78-456e-8182-a999aa453971",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_data.isnull().sum()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "abstract    0\n",
              "keyword     0\n",
              "title       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSIRok0-tIaw",
        "outputId": "68cb7e35-7753-4a75-f450-ff8c03a17573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>keyword</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>We investigate the problem of delay constraine...</td>\n",
              "      <td>algorithms;design;performance;sensor networks;...</td>\n",
              "      <td>Real-Time Data Aggregation in Contention-Based...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This paper describes a method for detecting ev...</td>\n",
              "      <td>biomedical text;machine learning;information e...</td>\n",
              "      <td>word sense disambiguation for event trigger wo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The lack of architecturally-significant mechan...</td>\n",
              "      <td>architectural styles;architectural aspects;poi...</td>\n",
              "      <td>composing architectural aspects based on style...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This paper describes our use of pen-based elec...</td>\n",
              "      <td>computer science;present;groupware;use;technol...</td>\n",
              "      <td>using pen-based computers across the computer ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>We show how to connect the syntactic and the f...</td>\n",
              "      <td>operational semantics;program transformation;r...</td>\n",
              "      <td>On the syntactic and functional correspondence...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>This paper presents a field-programmable gate ...</td>\n",
              "      <td>boolean satisfiability (sat);field-programmabl...</td>\n",
              "      <td>FPGA PLB architecture evaluation and area opti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>A simple model of inductor-coupled bistable os...</td>\n",
              "      <td>bistable oscillators;inductor coupling;pulse p...</td>\n",
              "      <td>Pulse wave propagation in a large number of co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>We present a novel technique for encoding and ...</td>\n",
              "      <td>constant weight codes;encoding algorithms;diss...</td>\n",
              "      <td>A Coding Algorithm for Constant Weight Vectors...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>We combine multilayer perceptrons and self-org...</td>\n",
              "      <td>bankruptcy prediction;financial crisis;multila...</td>\n",
              "      <td>Bankruptcy visualization and prediction using ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>This paper presents an automatic grading syste...</td>\n",
              "      <td>grading system;beef marbling;texture analysis;...</td>\n",
              "      <td>On a grading system for beef marbling ?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                abstract  ...                                              title\n",
              "0      We investigate the problem of delay constraine...  ...  Real-Time Data Aggregation in Contention-Based...\n",
              "1      This paper describes a method for detecting ev...  ...  word sense disambiguation for event trigger wo...\n",
              "2      The lack of architecturally-significant mechan...  ...  composing architectural aspects based on style...\n",
              "3      This paper describes our use of pen-based elec...  ...  using pen-based computers across the computer ...\n",
              "4      We show how to connect the syntactic and the f...  ...  On the syntactic and functional correspondence...\n",
              "...                                                  ...  ...                                                ...\n",
              "19995  This paper presents a field-programmable gate ...  ...  FPGA PLB architecture evaluation and area opti...\n",
              "19996  A simple model of inductor-coupled bistable os...  ...  Pulse wave propagation in a large number of co...\n",
              "19997  We present a novel technique for encoding and ...  ...  A Coding Algorithm for Constant Weight Vectors...\n",
              "19998  We combine multilayer perceptrons and self-org...  ...  Bankruptcy visualization and prediction using ...\n",
              "19999  This paper presents an automatic grading syste...  ...            On a grading system for beef marbling ?\n",
              "\n",
              "[20000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuhroJ0kVZmJ",
        "outputId": "18efab24-684b-4ed0-9154-5d90f03b3c4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "#test_data['TextScoring'] = test_data['abstract'].apply(lambda x: \",\".join(TextScoring(x)))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 8.11 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TALaRZr0r_so"
      },
      "source": [
        "#test_data['keyword'] = test_data['keyword'].str.replace(';',',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isiMWnqXVUG9"
      },
      "source": [
        "#test_data['keyword'] = test_data.keyword.apply(lambda x: x.strip(';').split(','))\n",
        "#test_data['TextScoring'] = test_data.TextScoring.apply(lambda x: x.strip(',').split(','))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeooPuj6VUKE",
        "outputId": "a92b1ead-2fe7-42ae-c483-9b5fbd50be5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "#evaluate(test_data['keyword'],test_data['TextScoring'],test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "########################\n",
            "Metrics\n",
            "@1\n",
            "F1:0.0\n",
            "P:0.0\n",
            "R:0.0\n",
            "@3\n",
            "F1:0.0\n",
            "P:0.0\n",
            "R:0.0\n",
            "@5\n",
            "F1:0.0\n",
            "P:0.0\n",
            "R:0.0\n",
            "@10\n",
            "F1:0.0\n",
            "P:0.0\n",
            "R:0.0\n",
            "@30\n",
            "F1:0.0\n",
            "P:0.0\n",
            "R:0.0\n",
            "#########################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nubQGbRMVUDZ",
        "outputId": "7244f8e5-e801-495e-d35c-159e451b3387",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip '/content/drive/My Drive/Colab Notebooks/Advanced NLP-Project/Project/all_title_abstract_keyword_clean.json.zip'"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Colab Notebooks/Advanced NLP-Project/Project/all_title_abstract_keyword_clean.json.zip\n",
            "  inflating: all_title_abstract_keyword_clean.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbO3s-ASxJeV"
      },
      "source": [
        "import json \n",
        "all_data = []\n",
        "for line in open('/content/all_title_abstract_keyword_clean.json', 'r'):\n",
        "    all_data.append(json.loads(line))"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5YTEKQpx7Oz"
      },
      "source": [
        "all_data = all_data[0]"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvJmcLf9HJdP"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of3s4pRTxT3E"
      },
      "source": [
        "all_data1 = pd.DataFrame(all_data)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq4erRzyxUbq",
        "outputId": "7c2e0591-133f-4e56-9640-4d6588fb124e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "all_data1.shape"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(578015, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XRKiOXcxUV2"
      },
      "source": [
        "all_data11 = all_data1.sample(20000)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "docEQKADWsWs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eiTH5FY3Asj",
        "outputId": "8c579936-1b51-4ed3-f350-bd78c4d964ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "all_data11['TextScoring'] = all_data11['abstract'].apply(lambda x: \",\".join(TextScoring(x)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1h 16min 15s, sys: 1min 8s, total: 1h 17min 24s\n",
            "Wall time: 1h 17min 32s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj_mUYh53LDq"
      },
      "source": [
        "all_data11['keyword'] = all_data11['keyword'].str.replace(';',',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdLXVNBs3LGQ"
      },
      "source": [
        "all_data11['keyword'] = all_data11.keyword.apply(lambda x: x.strip(';').split(','))\n",
        "all_data11['TextScoring'] = all_data11.TextScoring.apply(lambda x: x.strip(',').split(','))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUhFjdfpM6TP",
        "outputId": "59f7af3d-4ad3-43c1-e7cb-3d081a79b850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "all_data11"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>keyword</th>\n",
              "      <th>title</th>\n",
              "      <th>TextScoring</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>213624</th>\n",
              "      <td>Purpose - The purpose of this paper is to stud...</td>\n",
              "      <td>[induction, electromagnetic brake, electromagn...</td>\n",
              "      <td>New DC electromagnetic wiping system for hot-d...</td>\n",
              "      <td>[liquid zinc layer thanks, liquid zinc thanks,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431544</th>\n",
              "      <td>The intermolecular potential between a 18-crow...</td>\n",
              "      <td>[ab initio fitted potential, simulation, solva...</td>\n",
              "      <td>The hydration structure of 18-crown-6/K+ compl...</td>\n",
              "      <td>[water molecule, monte carlo simulation method...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>457042</th>\n",
              "      <td>The article formulates a dynamic mathematical ...</td>\n",
              "      <td>[production, consumption, multilateral exchang...</td>\n",
              "      <td>The dynamics of multilateral exchange</td>\n",
              "      <td>[many player produce, many good, third player,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>537182</th>\n",
              "      <td>We model and solve the problems of preemptive ...</td>\n",
              "      <td>[scheduling, parallel machines, uniform machin...</td>\n",
              "      <td>A minimum-cost network flow approach to preemp...</td>\n",
              "      <td>[minimum-cost network flow problem, correspond...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233154</th>\n",
              "      <td>In deep submicron designs, the interconnect wi...</td>\n",
              "      <td>[static timing analysis, gate delay, cmos inve...</td>\n",
              "      <td>A novel model for computing the effective capa...</td>\n",
              "      <td>[nonlinear gate output, effective capacitance ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406737</th>\n",
              "      <td>This paper describes an exploratory study to i...</td>\n",
              "      <td>[student-student interaction, student experien...</td>\n",
              "      <td>exploring factors that influence computer scie...</td>\n",
              "      <td>[student factor best predict intention, resear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355931</th>\n",
              "      <td>This study discusses preservice teachers' achi...</td>\n",
              "      <td>[pedagogy and technology, preservice teacher e...</td>\n",
              "      <td>Understanding preservice teachers' technology ...</td>\n",
              "      <td>[technological pedagogical content knowledge, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551380</th>\n",
              "      <td>In this paper, signal processing techniques wh...</td>\n",
              "      <td>[signal processing, automatic speech recogniti...</td>\n",
              "      <td>Signal processing techniques for robust speech...</td>\n",
              "      <td>[suitable signal processing technique, signal ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431264</th>\n",
              "      <td>Designing low end-to-end latency system archit...</td>\n",
              "      <td>[virtual reality, image-based rendering, latency]</td>\n",
              "      <td>A shared-scene-graph image-warping architectur...</td>\n",
              "      <td>[low end-to-end latency system architecture, i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243866</th>\n",
              "      <td>Selective sampling is a form of active learnin...</td>\n",
              "      <td>[coevolution, active learning, eea, sampling]</td>\n",
              "      <td>informative sampling for large unbalanced data...</td>\n",
              "      <td>[training data reveals, medical data set, trai...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 abstract  ...                                        TextScoring\n",
              "213624  Purpose - The purpose of this paper is to stud...  ...  [liquid zinc layer thanks, liquid zinc thanks,...\n",
              "431544  The intermolecular potential between a 18-crow...  ...  [water molecule, monte carlo simulation method...\n",
              "457042  The article formulates a dynamic mathematical ...  ...  [many player produce, many good, third player,...\n",
              "537182  We model and solve the problems of preemptive ...  ...  [minimum-cost network flow problem, correspond...\n",
              "233154  In deep submicron designs, the interconnect wi...  ...  [nonlinear gate output, effective capacitance ...\n",
              "...                                                   ...  ...                                                ...\n",
              "406737  This paper describes an exploratory study to i...  ...  [student factor best predict intention, resear...\n",
              "355931  This study discusses preservice teachers' achi...  ...  [technological pedagogical content knowledge, ...\n",
              "551380  In this paper, signal processing techniques wh...  ...  [suitable signal processing technique, signal ...\n",
              "431264  Designing low end-to-end latency system archit...  ...  [low end-to-end latency system architecture, i...\n",
              "243866  Selective sampling is a form of active learnin...  ...  [training data reveals, medical data set, trai...\n",
              "\n",
              "[20000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKwq_brUNXDK",
        "outputId": "ec7b03b5-5557-4c26-c339-373a40a9ee59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "all_data11.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "abstract       0\n",
              "keyword        0\n",
              "title          0\n",
              "TextScoring    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74jD2GoW3LLb"
      },
      "source": [
        "def evaluate(candidates, references, data):\n",
        "    precision_scores, recall_scores, f1_scores = {1:[], 3:[], 5:[], 10:[], 30:[]}, {1:[], 3:[], 5:[], 10:[], 30:[]}, {1:[], 3:[], 5:[], 10:[], 30:[]}\n",
        "    for url in range(len(data)):\n",
        "        candidate = remove_empty(candidates.iloc[url])\n",
        "        reference = remove_empty(references.iloc[url])\n",
        "        p, r = get_score_full(candidate, reference) \n",
        "        for i in [1,3,5,10,30]:\n",
        "            precision = p[i-1]\n",
        "            recall = r[i-1]\n",
        "            if precision + recall > 0:\n",
        "                f1_scores[i].append((2 * (precision * recall)) / (precision + recall))\n",
        "            else:\n",
        "                f1_scores[i].append(0)\n",
        "            precision_scores[i].append(precision)\n",
        "            recall_scores[i].append(recall)\n",
        "    print(\"########################\\nMetrics\")\n",
        "    for i in precision_scores:\n",
        "        print(\"@{}\".format(i))\n",
        "        print(\"F1:{}\".format(np.mean(f1_scores[i])))\n",
        "        print(\"P:{}\".format(np.mean(precision_scores[i])))\n",
        "        print(\"R:{}\".format(np.mean(recall_scores[i])))\n",
        "    print(\"#########################\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB78xcx23LI5",
        "outputId": "af3ce641-1fe6-488c-aafb-26c118f6947d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "evaluate(all_data11['TextScoring'],all_data11['keyword'],all_data11)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "########################\n",
            "Metrics\n",
            "@1\n",
            "F1:0.012993202211297291\n",
            "P:0.0357\n",
            "R:0.008161493688695167\n",
            "@3\n",
            "F1:0.03757045856962576\n",
            "P:0.0482\n",
            "R:0.03253647716411565\n",
            "@5\n",
            "F1:0.05067614304862105\n",
            "P:0.04989\n",
            "R:0.0552404362214484\n",
            "@10\n",
            "F1:0.06080353974223194\n",
            "P:0.04614\n",
            "R:0.09869307090647352\n",
            "@30\n",
            "F1:0.04601481791677741\n",
            "P:0.027956666666666668\n",
            "R:0.16356677237698772\n",
            "#########################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGy-Vjtczq8j"
      },
      "source": [
        "data_1000 = all_data1.sample(2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw7nYBQez6mN",
        "outputId": "b048b15d-28a9-4163-b0da-e97ea94a4a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "data_1000['TextScoring'] = data_1000['abstract'].apply(lambda x: \",\".join(TextScoring(x)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7min 37s, sys: 282 ms, total: 7min 37s\n",
            "Wall time: 7min 39s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H6m6bitz6p9"
      },
      "source": [
        "data_1000['keyword'] = data_1000['keyword'].str.replace(';',',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nadeOZUz6ig"
      },
      "source": [
        "data_1000['keyword'] = data_1000.keyword.apply(lambda x: x.strip(';').split(','))\n",
        "data_1000['TextScoring'] = data_1000.TextScoring.apply(lambda x: x.strip(',').split(','))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDOKQLzF1I4K",
        "outputId": "25e03834-334d-482d-cf09-c48acea7c613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "evaluate(data_1000['TextScoring'],data_1000['keyword'],data_1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "########################\n",
            "Metrics\n",
            "@1\n",
            "F1:0.011723412698412698\n",
            "P:0.031\n",
            "R:0.007378787878787879\n",
            "@3\n",
            "F1:0.040255180974939034\n",
            "P:0.049999999999999996\n",
            "R:0.035182545232545236\n",
            "@5\n",
            "F1:0.05223056415548676\n",
            "P:0.0506\n",
            "R:0.057313060238718135\n",
            "@10\n",
            "F1:0.058552584509061714\n",
            "P:0.04395\n",
            "R:0.09593110681171035\n",
            "@30\n",
            "F1:0.044498287357803576\n",
            "P:0.0269\n",
            "R:0.1597765103678985\n",
            "#########################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy39PVCgqVcR"
      },
      "source": [
        "########################\n",
        "Metrics\n",
        "@1\n",
        "F1:0.059900282565187916\n",
        "P:0.45\n",
        "R:0.03244000175888385\n",
        "@3\n",
        "F1:0.12487926811722572\n",
        "P:0.37\n",
        "R:0.07766035767202357\n",
        "@5\n",
        "F1:0.16587912407745475\n",
        "P:0.32600000000000007\n",
        "R:0.11712560332740846\n",
        "@10\n",
        "F1:0.2282296667787783\n",
        "P:0.28900000000000003\n",
        "R:0.20330807026384912\n",
        "@30\n",
        "F1:0.25265837540621766\n",
        "P:0.1975\n",
        "R:0.3895969630102131\n",
        "#########################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgKusLkiajjd",
        "outputId": "a8d38b30-1a72-45cf-cd30-fc8fa504e51e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "import json \n",
        "all_data = [] \n",
        "for line in open('/content/drive/My Drive/Colab Notebooks/Advanced NLP-Project/Project/KPTimes.test.jsonl', 'r'):\n",
        "    all_data.append(json.loads(line))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-824691e54736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Colab Notebooks/Advanced NLP-Project/Project/KPTimes.test.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/Advanced NLP-Project/Project/KPTimes.test.jsonl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIIXWtqa8OUr",
        "outputId": "76c46546-e8cd-4332-ab10-53181f9551fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "data_df = pd.DataFrame(all_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1b6ffe35c974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtGQHg-5_YNW",
        "outputId": "c80e0173-8060-43f8-f8b5-f10953f03083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "data_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-38ce564c9fb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAsiIycQ_gje"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w46m1IgDoRxw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-ykQjpjoSQ_"
      },
      "source": [
        "## TfIdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QklVZq0UoWFH"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "\n",
        "from orderedset import OrderedSet\n",
        "\n",
        "'''\n",
        "keywords extraction from a document using TF-IDF from scratch (completely from scratch). \n",
        "'''\n",
        "\n",
        "def clean(text):\n",
        "\n",
        "\ttext = text.lower() # convert all words to lower case, \n",
        "\t\n",
        "\ttext = re.sub(r'\\s+', ' ', text) # replace or substitute all spaces, tabs, indents (\\s) with ' ' (space) \n",
        "\n",
        "\ttext = re.sub(r'\\d', ' ', text) # replace all digits by ' '\n",
        "\n",
        "\ttext = re.sub(r'[^a-zA-Z. ]+', '', text) # replace all non words (\\W) with ' '. (note: small w is for all words. capital W is for all non-words)\n",
        "\t\n",
        "\t#print(text)\n",
        "\n",
        "\treturn text\n",
        "\n",
        "# list of all words\n",
        "def get_sentence_of_words(text):\n",
        "\n",
        "\tsentence = list() # list of sentences\n",
        "\twords = list() # list of words in each sentence.\n",
        "  sentence_list = list()\n",
        "  temp = text.strip().split(\". \") # temporary list of sentences. \n",
        "  for sent in temp:\n",
        "\n",
        "\t\twords = sent.strip().split(\" \") # getting the words in sentences.\n",
        "\n",
        "\t\twords = [i for i in words if len(i) > 1]\n",
        "\n",
        "\t\tif(len(words) > 1): \n",
        "\t\t\tsentence.append(words) # sentence is a list of lists. (contains a list of sentences in which each sentence is a list of words)\n",
        "\n",
        "\t\tsentence_list.append(sent)\t\n",
        "\n",
        "\t#print(sentence, len(sentence))\t\n",
        "\treturn sentence, sentence_list\n",
        "\n",
        "\n",
        "def vectorize(sentence):\n",
        "\n",
        "\t# set of unique words in the whole document.\n",
        "\tunique_words = OrderedSet() \n",
        "\n",
        "\tfor sent in sentence:\n",
        "\t\tfor word in sent:\n",
        "\t\t\t\n",
        "\t\t\tunique_words.add(word)\n",
        "\n",
        "\tunique_words = list(unique_words) # converting the set to a list to make it easier to work with it. \n",
        "\n",
        "\t#print(unique_words, len(unique_words))\n",
        "\n",
        "\t# a list of lists that contains the vectorized form of each sentence in the document. \n",
        "\tvector = list()\n",
        "\n",
        "\n",
        "\t# in the vectorized representation, we consider the bag of words (unique words in the text).\n",
        "\t# then, we count the occurenc of each word in a sentence and represent it in a vector whose length = length(unique_words)\n",
        "\t# ex: sent1 = \"i am a boy\"\n",
        "\t#     sent2 = \"i am a girl\"\n",
        "\t# unique_words = [\"i\", \"am\", \"a\", \"boy\", \"girl\"]\n",
        "\t# vector representation of sent1 = [1, 1, 1, 1, 0]\n",
        "\t# vector representation of sent2 = [1, 1, 1, 0, 1]\n",
        "\n",
        "\tfor sent in sentence: # iterate for every sentence in the document\n",
        "\t\ttemp_vector = [0] * len(unique_words) # create a temporary vector to calculate the occurence of each word in that sentence. \n",
        "\t\t\n",
        "\t\tfor word in sent: # iterate for every word in the sentence. \n",
        "\n",
        "\t\t\ttemp_vector[unique_words.index(word)] += 1\t\n",
        "\n",
        "\t\tvector.append(temp_vector) # add the temporary vector to the list of vectors for each sentence (list of lists)\n",
        "\n",
        "\t#print(vector)\t\n",
        "\n",
        "\n",
        "\treturn vector, unique_words\t\n",
        "\n",
        "# function to calculate the tf scores\n",
        "def tf(vector, sentence, unique_words):\n",
        "\n",
        "\ttf = list()\n",
        "\n",
        "\tno_of_unique_words = len(unique_words) \n",
        "\n",
        "\tfor i in range(len(sentence)):\n",
        "\n",
        "\t\ttflist = list()\n",
        "\t\tsent = sentence[i]\n",
        "\t\tcount = vector[i]\n",
        "\n",
        "\t\tfor word in sent:\n",
        "\t\t\t'''\n",
        "\t\t\tif(count[sent.index(word)] == 0):\n",
        "\t\t\t\tcount[sent.index(word)] = 1\n",
        "\t\t\t'''\n",
        "\t\t\tscore = count[sent.index(word)]/ float(len(sent)) # tf = no. of occurence of a word/ total no. of words in the sentence. \n",
        "\n",
        "\t\t\tif(score == 0):\n",
        "\t\t\t\tscore = 1/ float(len(sentence))\n",
        "\n",
        "\t\t\ttflist.append(score)  \n",
        "\n",
        "\t\ttf.append(tflist)\n",
        "\n",
        "\t# print(tf)\t\n",
        "\t\n",
        "\treturn tf\t\n",
        "\n",
        "\n",
        "#function to calculate idf. \n",
        "def idf(vector, sentence, unique_words):\n",
        "\n",
        "\t# idf = log(no. of sentences / no. of sentences in which the word appears).\n",
        "\n",
        "\tno_of_sentences = len(sentence)\n",
        "\n",
        "\tidf = list()\n",
        "\n",
        "\tfor sent in sentence:\n",
        "\t\t\n",
        "\t\tidflist = list()\n",
        "\n",
        "\t\tfor word in sent:\n",
        "\n",
        "\t\t\tcount = 0 # no. of times the word occurs in the entire text.\n",
        "\n",
        "\t\t\tfor k in sentence:\n",
        "\t\t\t\tif(word in k):\n",
        "\t\t\t\t\tcount += 1\n",
        "\t\t\n",
        "\n",
        "\t\t\tscore = math.log(no_of_sentences/float(count)) # caclulating idf scores\n",
        "\n",
        "\t\t\tidflist.append(score)\n",
        "\n",
        "\t\tidf.append(idflist)\t\n",
        "\n",
        "\t# print(idf)\t\n",
        "\n",
        "\treturn idf\n",
        "\n",
        "\n",
        "# function to calculate the tf-idf scores.\n",
        "def tf_idf(tf, idf):\n",
        "\n",
        "\t# tf-idf = tf(w) * idf(w)\n",
        "\n",
        "\ttfidf = [[0 for j in range(len(tf[i]))] for i in range(len(tf))]\n",
        "\n",
        "\tfor i in range(len(tf)):\n",
        "\t\tfor j in range(len(tf[i])):\n",
        "\n",
        "\t\t\ttfidf[i][j] = tf[i][j] * float(idf[i][j])\n",
        "\n",
        "\t# print(tfidf)\t\t\n",
        "\n",
        "\treturn tfidf\t\n",
        "\n",
        "\n",
        "def extract_keywords(tfidf, processed_text):\n",
        "\t\n",
        "\tmapping = {}\n",
        "\n",
        "\tfor i in range(len(tfidf)):\n",
        "\t\tfor j in range(len(tfidf[i])):\n",
        "\n",
        "\t\t\tmapping[processed_text[i][j]] = tfidf[i][j]\n",
        "\n",
        "\t#print(mapping)\n",
        "\n",
        "\tword_scores = sorted(mapping.values(), reverse = True)\n",
        "\twords = []\n",
        "\n",
        "\tscores_to_word = {}\n",
        "\n",
        "\tfor i in range(len(tfidf)):\n",
        "\t\tfor j in range(len(tfidf[i])):\n",
        "\n",
        "\t\t\tscores_to_word[tfidf[i][j]] = processed_text[i][j]\n",
        "\n",
        "\tfor i in range(len(word_scores)):\n",
        "\t\tif(word_scores[i] != 0):\n",
        "\t\t\twords.append(scores_to_word[word_scores[i]])\n",
        "\t\telse:\n",
        "\t\t\twords.append(scores_to_word[word_scores[i]])\n",
        "\t\t\tbreak\n",
        "\n",
        "\t# print(words)\t\n",
        "\n",
        "\twords = OrderedSet(words)\n",
        "\n",
        "\tfor i in mapping:\n",
        "\t\tif(mapping[i] == 0):\n",
        "\t\t\twords.append(i)\t\t\n",
        "\t\n",
        "\treturn words, mapping\n",
        "\n",
        "\n",
        "def save_keywords(words, mapping):\n",
        "\n",
        "\tscores = []\n",
        "\n",
        "\tfor word in words:\n",
        "\t\tscores.append(mapping[word])\n",
        "\n",
        "\t# print(words, scores)\n",
        "\n",
        "\td = {'Words': words, 'Scores': scores}\n",
        "\n",
        "\tdata = pd.DataFrame(d)\n",
        "\n",
        "\tdata.to_csv('keywords.csv', sep = '\\t')\n",
        "\n",
        "\tprint(data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsTVKlMe0J5x"
      },
      "source": [
        "data_1000_1 = data_1000.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiN0VY0S0FfR"
      },
      "source": [
        "data_1000_1.abstract = data_1000_1.abstract.apply(lambda x: clean(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2rJdKNNoavq"
      },
      "source": [
        "data_1000_1['processed_text'], data_1000_1['sentence_list'] = zip(*data_1000_1.abstract.apply(lambda x : get_sentence_of_words(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVHuHzUY38J-"
      },
      "source": [
        "lst = list(data_1000_1.sentence_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbAscifu4CYZ",
        "outputId": "d9776da9-90a7-4772-c85c-0da99a3eb346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "sentence_to_index = defaultdict(list)\n",
        "\n",
        "for k, i in enumerate(lst):\n",
        "  sentence_to_index[i].append(k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-987d8b3313f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0msentence_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRWo8e3c15vX",
        "outputId": "171abfdd-9a3c-446d-c66f-98d8c2ad7b1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "sentence_to_index = defaultdict(list)\n",
        "for k, i in enumerate(list(data_1000_1.sentence_list)):\n",
        "  sentence_to_index[i].append(k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-66c2e1b260e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentence_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1000_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0msentence_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kd_xEKM2oL4"
      },
      "source": [
        "data_1000_1.sentence_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swim9Ifpo0Op",
        "outputId": "d20814ce-7230-403f-b079-9d36fac23b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "sentence_to_index = {i:k for k, i in enumerate(list(data_1000_1.sentence_list))}\n",
        "\n",
        "# vector, unique_words = vectorize(processed_text)\n",
        "\n",
        "# tf = tf(vector, processed_text, unique_words)\n",
        "\n",
        "# idf = idf(vector, processed_text, unique_words)\n",
        "\n",
        "# tfidf = tf_idf(tf, idf)\t\n",
        "\n",
        "# keywords, mapping = extract_keywords(tfidf, processed_text)\n",
        "\n",
        "# #save_keywords(keywords, mapping)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-90146d5c3f0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1000_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# vector, unique_words = vectorize(processed_text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# tf = tf(vector, processed_text, unique_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-90146d5c3f0c>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1000_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# vector, unique_words = vectorize(processed_text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# tf = tf(vector, processed_text, unique_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K10P7qBD1QQ9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPE3w9lKPBQM",
        "outputId": "5f78475a-b274-47f9-d7db-6c4dd49208e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "import re\n",
        "def pre_process(text):\n",
        "    \n",
        "    # lowercase\n",
        "    text=text.lower()\n",
        "    \n",
        "    #remove tags\n",
        "    text=re.sub(\"</?.*?>\",\" <> \",text)\n",
        "    \n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "data_1000_1['abstract'] = data_1000_1['abstract'].apply(lambda x:pre_process(x))\n",
        "\n",
        "#show the first 'text'\n",
        "data_1000_1['abstract'][2]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'in this paper we discuss the motivation and the fundamentals of an ontology representation of business reporting data and metadata structures as defined in the extensible business reporting language xbrl standard the core motivation for an ontology representation is the enhanced potential for integrated analytic applications that build on quantitative reporting data combined with structured and unstructured data from additional sources applications of this kind will enable significant enhancements in regulatory compliance management as they enable business analytics combined with inference engines for statistical but also for logical inferences in order to define a suitable ontology representation of business reporting language structures an analysis of the logical principles of the reporting metadata taxonomies and further classification systems is presented based on this analysis a representation of the generally accepted accounting principles taxonomies in xbrl by an ontology provided in the web ontology language owl is proposed an additional advantage of this representation is its compliance with the recent ontology definition metamodel odm standard issued by omg '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_l5kI1BPBNi"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "\n",
        "def get_stop_words(stop_file_path):\n",
        "    \"\"\"load stop words \"\"\"\n",
        "    \n",
        "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "        stopwords = f.readlines()\n",
        "        stop_set = set(m.strip() for m in stopwords)\n",
        "        return frozenset(stop_set)\n",
        "\n",
        "#load a set of stop words\n",
        "stopwords=get_stop_words(\"/content/drive/My Drive/Colab Notebooks/Advanced NLP-Project/Project/sklearn_stopwords.txt\")\n",
        "\n",
        "#get the text column \n",
        "docs=data_1000_1['abstract'].tolist()\n",
        "\n",
        "#create a vocabulary of words, \n",
        "#ignore words that appear in 85% of documents, \n",
        "#eliminate stop words\n",
        "cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
        "word_count_vector=cv.fit_transform(docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMHH9engPBKr",
        "outputId": "e45d2e24-8b1b-470b-bc38-74ce122aa7fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_count_vector.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 12614)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzw2VSKtPgDH",
        "outputId": "6d9cc658-8f58-40a0-e613-3adc05d66094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cv=CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=10000)\n",
        "word_count_vector=cv.fit_transform(docs)\n",
        "word_count_vector.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBkIAG_7PgA7",
        "outputId": "4a28b805-da9a-4af1-ec90-2609d38a3f6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "list(cv.vocabulary_.keys())[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['paper',\n",
              " 'proposes',\n",
              " 'using',\n",
              " 'virtual',\n",
              " 'reality',\n",
              " 'enhance',\n",
              " 'perception',\n",
              " 'actions',\n",
              " 'distant',\n",
              " 'users']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT4kDF4VPf-s",
        "outputId": "497611ab-1a5c-41c9-d955-1277dac7ee8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "list(cv.get_feature_names())[2000:2015]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dataspace',\n",
              " 'datatype',\n",
              " 'datavalue',\n",
              " 'date',\n",
              " 'dates',\n",
              " 'daunorubicin',\n",
              " 'davidov',\n",
              " 'day',\n",
              " 'dayneka',\n",
              " 'days',\n",
              " 'db',\n",
              " 'dbc',\n",
              " 'dbms',\n",
              " 'dbn',\n",
              " 'dbpedia']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS0YtxoFPf8c",
        "outputId": "499ec2ef-d3a3-48cc-b8dc-9b9a54664da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(word_count_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHqQyrpXPptP",
        "outputId": "24d55a0d-5c9b-494d-d27c-88bf9f312509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "tfidf_transformer.idf_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7.2156076, 7.2156076, 7.2156076, ..., 7.2156076, 7.2156076,\n",
              "       7.2156076])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0NgB5PuPpqC"
      },
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    \n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "\n",
        "    for idx, score in sorted_items:\n",
        "        fname = feature_names[idx]\n",
        "        \n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIJoKqNiPf5o",
        "outputId": "66e4aee4-603d-4d8b-fd43-50e479818536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# you only needs to do this once\n",
        "feature_names=cv.get_feature_names()\n",
        "\n",
        "# get the document that we want to extract keywords from\n",
        "doc=docs[0]\n",
        "\n",
        "#generate tf-idf for the given document\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
        "\n",
        "#sort the tf-idf vectors by descending order of scores\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "#extract only the top n; n here is 10\n",
        "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "\n",
        "# now print the results\n",
        "for k in keywords:\n",
        "    print(k,keywords[k])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actions 0.468\n",
            "playback 0.275\n",
            "recorded 0.268\n",
            "collaboration 0.257\n",
            "remote 0.244\n",
            "shared 0.228\n",
            "application 0.226\n",
            "virtual 0.209\n",
            "user 0.169\n",
            "space 0.167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcp-CxiaPf3C"
      },
      "source": [
        "# put the common code into several methods\n",
        "def get_keywords(idx):\n",
        "\n",
        "    #generate tf-idf for the given document\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n",
        "\n",
        "    #sort the tf-idf vectors by descending order of scores\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "    \n",
        "    return keywords\n",
        "\n",
        "def print_results(idx,keywords):\n",
        "    # now print the results\n",
        "    print(\"\\n===Keywords===\")\n",
        "    for k in keywords:\n",
        "        print(k,keywords[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCshz7R7QUyG",
        "outputId": "4466a4c4-c118-4f3f-9618-6915c12f686c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "idx=120\n",
        "keywords=get_keywords(idx)\n",
        "print_results(idx,keywords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "===Keywords===\n",
            "mutation 0.628\n",
            "analysis 0.284\n",
            "current 0.198\n",
            "highlighted 0.179\n",
            "reviewed 0.166\n",
            "suffer 0.157\n",
            "outlined 0.157\n",
            "automation 0.154\n",
            "unfortunately 0.15\n",
            "severe 0.148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUb-kJtdQWCI",
        "outputId": "4467308d-7764-4df7-900a-7088c5255561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "\n",
        "#generate tf-idf for all documents in your list. docs_test has 500 documents\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform(docs))\n",
        "\n",
        "results=[]\n",
        "for i in range(tf_idf_vector.shape[0]):\n",
        "    \n",
        "    # get vector for a single document\n",
        "    curr_vector=tf_idf_vector[i]\n",
        "    \n",
        "    #sort the tf-idf vector by descending order of scores\n",
        "    sorted_items=sort_coo(curr_vector.tocoo())\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,100)\n",
        "    \n",
        "    \n",
        "    results.append(keywords)\n",
        "\n",
        "df=pd.DataFrame(zip(docs,results),columns=['doc','keywords'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>this paper proposes using virtual reality to e...</td>\n",
              "      <td>{'actions': 0.468, 'playback': 0.275, 'recorde...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this paper presents an improved architecture o...</td>\n",
              "      <td>{'modulator': 0.383, 'oscillation': 0.339, 'hq...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>in this paper we discuss the motivation and th...</td>\n",
              "      <td>{'ontology': 0.441, 'reporting': 0.428, 'busin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>an overview of the selforganizing map algorith...</td>\n",
              "      <td>{'selforganizing': 0.496, 'papers': 0.425, 'ov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the amygdala comprises part of an extended net...</td>\n",
              "      <td>{'social': 0.44, 'disruption': 0.224, 'amygdal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>because of its simplicity and intuitive approa...</td>\n",
              "      <td>{'rendering': 0.467, 'visibility': 0.456, 'poi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>this study observed the expression of transfor...</td>\n",
              "      <td>{'pheo': 0.566, 'tnf': 0.471, 'tgf': 0.377, 'p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>threedimensional simulation models are hard to...</td>\n",
              "      <td>{'simulation': 0.314, 'interactive': 0.232, 'v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>nearest neighbor nn search in highdimensional ...</td>\n",
              "      <td>{'query': 0.234, 'space': 0.23, 'lsh': 0.223, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>recent years coverage has been widely investig...</td>\n",
              "      <td>{'coverage': 0.508, 'sensors': 0.29, 'algorith...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   doc                                           keywords\n",
              "0    this paper proposes using virtual reality to e...  {'actions': 0.468, 'playback': 0.275, 'recorde...\n",
              "1    this paper presents an improved architecture o...  {'modulator': 0.383, 'oscillation': 0.339, 'hq...\n",
              "2    in this paper we discuss the motivation and th...  {'ontology': 0.441, 'reporting': 0.428, 'busin...\n",
              "3    an overview of the selforganizing map algorith...  {'selforganizing': 0.496, 'papers': 0.425, 'ov...\n",
              "4    the amygdala comprises part of an extended net...  {'social': 0.44, 'disruption': 0.224, 'amygdal...\n",
              "..                                                 ...                                                ...\n",
              "995  because of its simplicity and intuitive approa...  {'rendering': 0.467, 'visibility': 0.456, 'poi...\n",
              "996  this study observed the expression of transfor...  {'pheo': 0.566, 'tnf': 0.471, 'tgf': 0.377, 'p...\n",
              "997  threedimensional simulation models are hard to...  {'simulation': 0.314, 'interactive': 0.232, 'v...\n",
              "998  nearest neighbor nn search in highdimensional ...  {'query': 0.234, 'space': 0.23, 'lsh': 0.223, ...\n",
              "999  recent years coverage has been widely investig...  {'coverage': 0.508, 'sensors': 0.29, 'algorith...\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6u3mnCmgFda",
        "outputId": "c29ed0cb-9da3-4327-d077-9075cdae752b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        }
      },
      "source": [
        "data_1000_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>keyword</th>\n",
              "      <th>title</th>\n",
              "      <th>abs_keyword_count</th>\n",
              "      <th>TextScoring</th>\n",
              "      <th>processed_text</th>\n",
              "      <th>sentence_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>this paper proposes using virtual reality to e...</td>\n",
              "      <td>[telepresence, animation, avatars, application...</td>\n",
              "      <td>virtually enhancing the perception of user act...</td>\n",
              "      <td>5</td>\n",
              "      <td>[user action, recorded action, remote synchron...</td>\n",
              "      <td>[[this, paper, proposes, using, virtual, reali...</td>\n",
              "      <td>[this paper proposes using virtual reality to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this paper presents an improved architecture o...</td>\n",
              "      <td>[sigma delta modulators, analog-to-digital con...</td>\n",
              "      <td>Dynamic range improvement of multistage multib...</td>\n",
              "      <td>5</td>\n",
              "      <td>[leakage quantization noise problem, in-band q...</td>\n",
              "      <td>[[this, paper, presents, an, improved, archite...</td>\n",
              "      <td>[this paper presents an improved architecture ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>in this paper we discuss the motivation and th...</td>\n",
              "      <td>[enterprise information integration and intero...</td>\n",
              "      <td>An ontology modelling perspective on business ...</td>\n",
              "      <td>5</td>\n",
              "      <td>[business reporting language structure, extens...</td>\n",
              "      <td>[[in, this, paper, we, discuss, the, motivatio...</td>\n",
              "      <td>[in this paper we discuss the motivation and t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>an overview of the selforganizing map algorith...</td>\n",
              "      <td>[self-organizing map, learning vector quantiza...</td>\n",
              "      <td>The self-organizing map</td>\n",
              "      <td>2</td>\n",
              "      <td>[self-organizing map algorithm, paper, overvie...</td>\n",
              "      <td>[[an, overview, of, the, selforganizing, map, ...</td>\n",
              "      <td>[an overview of the selforganizing map algorit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the amygdala comprises part of an extended net...</td>\n",
              "      <td>[social brain, amygdala, behavior, facial expr...</td>\n",
              "      <td>The Amygdala and Development of the Social Brain</td>\n",
              "      <td>4</td>\n",
              "      <td>[social cognitive network, social behavior app...</td>\n",
              "      <td>[[the, amygdala, comprises, part, of, an, exte...</td>\n",
              "      <td>[the amygdala comprises part of an extended ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>because of its simplicity and intuitive approa...</td>\n",
              "      <td>[display algorithms, point-based rendering, de...</td>\n",
              "      <td>Real-time point-based rendering using visibili...</td>\n",
              "      <td>4</td>\n",
              "      <td>[extended point-based rendering method, distan...</td>\n",
              "      <td>[[because, of, its, simplicity, and, intuitive...</td>\n",
              "      <td>[because of its simplicity and intuitive appro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>this study observed the expression of transfor...</td>\n",
              "      <td>[pheochromocytoma, tgf-?, tnf-?, proliferation...</td>\n",
              "      <td>Expression and Effect of Transforming Growth F...</td>\n",
              "      <td>6</td>\n",
              "      <td>[primary human pheo cell, human pheo cell, phe...</td>\n",
              "      <td>[[this, study, observed, the, expression, of, ...</td>\n",
              "      <td>[this study observed the expression of transfo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>threedimensional simulation models are hard to...</td>\n",
              "      <td>[visualisation, ising model, cuda, gpu, instru...</td>\n",
              "      <td>Interactive visualisation of spins and cluster...</td>\n",
              "      <td>5</td>\n",
              "      <td>[computational simulation model, other visual ...</td>\n",
              "      <td>[[threedimensional, simulation, models, are, h...</td>\n",
              "      <td>[threedimensional simulation models are hard t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>nearest neighbor nn search in highdimensional ...</td>\n",
              "      <td>[theory, algorithms, experimentation, locality...</td>\n",
              "      <td>Efficient and Accurate Nearest Neighbor and Cl...</td>\n",
              "      <td>6</td>\n",
              "      <td>[high-dimensional nn search, strong quality gu...</td>\n",
              "      <td>[[nearest, neighbor, nn, search, in, highdimen...</td>\n",
              "      <td>[nearest neighbor nn search in highdimensional...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>recent years coverage has been widely investig...</td>\n",
              "      <td>[genetic algorithm, coverage compensation, nod...</td>\n",
              "      <td>a performance evaluation of a coverage compens...</td>\n",
              "      <td>5</td>\n",
              "      <td>[optimal coverage node distribution, coverage ...</td>\n",
              "      <td>[[recent, years, coverage, has, been, widely, ...</td>\n",
              "      <td>[recent years coverage has been widely investi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              abstract  ...                                      sentence_list\n",
              "0    this paper proposes using virtual reality to e...  ...  [this paper proposes using virtual reality to ...\n",
              "1    this paper presents an improved architecture o...  ...  [this paper presents an improved architecture ...\n",
              "2    in this paper we discuss the motivation and th...  ...  [in this paper we discuss the motivation and t...\n",
              "3    an overview of the selforganizing map algorith...  ...  [an overview of the selforganizing map algorit...\n",
              "4    the amygdala comprises part of an extended net...  ...  [the amygdala comprises part of an extended ne...\n",
              "..                                                 ...  ...                                                ...\n",
              "995  because of its simplicity and intuitive approa...  ...  [because of its simplicity and intuitive appro...\n",
              "996  this study observed the expression of transfor...  ...  [this study observed the expression of transfo...\n",
              "997  threedimensional simulation models are hard to...  ...  [threedimensional simulation models are hard t...\n",
              "998  nearest neighbor nn search in highdimensional ...  ...  [nearest neighbor nn search in highdimensional...\n",
              "999  recent years coverage has been widely investig...  ...  [recent years coverage has been widely investi...\n",
              "\n",
              "[1000 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orZbtFfqhNtl"
      },
      "source": [
        "df['keys'] = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcsW5VOufm1Y"
      },
      "source": [
        "keys = []\n",
        "for i in range(len(df)):\n",
        "  keys1 = []\n",
        "  keys1.append(df['keywords'].iloc[i].keys())\n",
        "  keys.append(keys1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfqOyQxIf43E"
      },
      "source": [
        "tfidf = pd.DataFrame(keys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyjSSMS-hrgf"
      },
      "source": [
        "tfidf.rename(columns={0: \"tfidf\"},inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF4XbRAeiE-b",
        "outputId": "94de1ef1-a43d-4342-8f9d-eedbf74fd2b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "tfidf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(actions, playback, recorded, collaboration, r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(modulator, oscillation, hqcrffbased, quantiza...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(ontology, reporting, business, representation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(selforganizing, papers, overview, map, issue,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(social, disruption, amygdala, cortex, stimuli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>(rendering, visibility, pointbased, depthbased...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>(pheo, tnf, tgf, proliferation, apoptosis, tis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>(simulation, interactive, visual, models, syst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>(query, space, lsh, lsbtree, guarantees, magni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>(coverage, sensors, algorithms, centralized, d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 tfidf\n",
              "0    (actions, playback, recorded, collaboration, r...\n",
              "1    (modulator, oscillation, hqcrffbased, quantiza...\n",
              "2    (ontology, reporting, business, representation...\n",
              "3    (selforganizing, papers, overview, map, issue,...\n",
              "4    (social, disruption, amygdala, cortex, stimuli...\n",
              "..                                                 ...\n",
              "995  (rendering, visibility, pointbased, depthbased...\n",
              "996  (pheo, tnf, tgf, proliferation, apoptosis, tis...\n",
              "997  (simulation, interactive, visual, models, syst...\n",
              "998  (query, space, lsh, lsbtree, guarantees, magni...\n",
              "999  (coverage, sensors, algorithms, centralized, d...\n",
              "\n",
              "[1000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ixqXwsUiYFO",
        "outputId": "5a8b07cf-fb58-43a7-d727-4815c54ec828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "evaluate(tfidf['tfidf'],data_1000_1['keyword'],data_1000_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "########################\n",
            "Metrics\n",
            "@1\n",
            "F1:0.33333333333333337\n",
            "P:1.0\n",
            "R:0.2\n",
            "@3\n",
            "F1:0.25\n",
            "P:0.3333333333333333\n",
            "R:0.2\n",
            "@5\n",
            "F1:0.20000000000000004\n",
            "P:0.2\n",
            "R:0.2\n",
            "@10\n",
            "F1:0.13333333333333333\n",
            "P:0.1\n",
            "R:0.2\n",
            "@30\n",
            "F1:0.05714285714285715\n",
            "P:0.03333333333333333\n",
            "R:0.2\n",
            "#########################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMO96xt-i6Qm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}