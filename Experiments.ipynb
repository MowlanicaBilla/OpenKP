{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "model2 = gensim.models.KeyedVectors.load_word2vec_format('D:/Word embedding/GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "import evaluate\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    l = []\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "        l.append(subtree.leaves())\n",
    "    \n",
    "    return l\n",
    "\n",
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    #word = stemmer.stem(word)\n",
    "    #word = lemmatizer.lemmatize(word)\n",
    "    return word\n",
    "\n",
    "def acceptable_word(word):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "    accepted = bool(2 <= len(word) <= 40\n",
    "        and word.lower() not in stopwords)\n",
    "    return accepted\n",
    "\n",
    "\n",
    "def get_terms(tree):\n",
    "    kp = []\n",
    "    for leaf in leaves(tree):\n",
    "        term = [ normalise(w) for w,t in leaf if acceptable_word(w) ]\n",
    "        if term:\n",
    "            kp.append(term)\n",
    "    \n",
    "    return kp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_kp(text):\n",
    "    \n",
    "    # Used when tokenizing words\n",
    "    sentence_re = r'''(?x)      # set flag to allow verbose regexps\n",
    "            (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "          | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "          | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "          | \\.\\.\\.              # ellipsis\n",
    "          | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "        '''\n",
    "\n",
    "    #lemmatizer = nltk.WordNetLemmatizer()\n",
    "    #stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "    #Taken from Su Nam Kim Paper\n",
    "    grammar = r\"\"\"\n",
    "        NBAR:\n",
    "            {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "\n",
    "        NP:\n",
    "            {<NBAR>}\n",
    "            {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "    \"\"\"\n",
    "\n",
    "    #toks = nltk.regexp_tokenize(text, sentence_re)\n",
    "    postoks = nltk.tag.pos_tag(text)\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    tree = chunker.parse(postoks)\n",
    "    terms = get_terms(tree)\n",
    "    pos,pos_set = find_positions(text,terms)\n",
    "    return terms,pos,pos_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'Inspec/docsutf8/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = 'Inspec/keys/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(data)\n",
    "key_files = os.listdir(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(document, kps):\n",
    "    ''' \n",
    "    merge the same kps & keep present kps in document\n",
    "    Inputs:\n",
    "        document : a word list : ['sun', 'sunshine', ...] || lower cased\n",
    "        kps : can have more than one kp : [['sun'], ['key','phrase'], ['sunshine']] || not duplicate\n",
    "    Outputs:\n",
    "        all_present_kps : present keyphrases\n",
    "        positions_for_all : start_end_posisiton for prensent keyphrases\n",
    "        a present kp postions list : every present's positions in documents, \n",
    "        each kp can be presented in several postions .\n",
    "        [[[0,0],[20,21]], [[1,1]]]\n",
    "    '''\n",
    "    tot_doc_char = ' '.join(document)\n",
    "    \n",
    "    positions_for_all = []\n",
    "    position_start,position_end =[],[]\n",
    "    all_present_kps = []\n",
    "    for kp in kps:\n",
    "        ans_string = ' '.join(kp)\n",
    "        \n",
    "        if ans_string not in tot_doc_char:\n",
    "            continue\n",
    "        else: \n",
    "            positions_for_each = []\n",
    "            # find all positions for each kp\n",
    "            for i in range(0, len(document) - len(kp) + 1):\n",
    "                \n",
    "                Flag = False\n",
    "                if kp == document[i:i+len(kp)]:\n",
    "                    Flag = True\n",
    "                if Flag:\n",
    "                    assert len(kp) >= 1\n",
    "                    positions_for_each.append((i+1, i+len(kp)))\n",
    "                    position_start.append(i+1)\n",
    "                    position_end.append(i+len(kp))\n",
    "        if len(positions_for_each) > 0 :\n",
    "            positions_for_all.extend(positions_for_each)\n",
    "            all_present_kps.append(kp)\n",
    "           \n",
    "    assert len(positions_for_all) >= len(all_present_kps)\n",
    "    \n",
    "    if len(all_present_kps) == 0:\n",
    "        return None\n",
    "    return [position_start,position_end],set(positions_for_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    no_punct=[words for words in text if words not in string.punctuation]\n",
    "    words_wo_punct=''.join(no_punct)\n",
    "    return words_wo_punct\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text=[word for word in text if word not in stopword]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(y_labels,y_preds,depth,levels):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    \n",
    "    for idx,y_label in enumerate(y_labels):\n",
    "        tp = 0\n",
    "        p = []\n",
    "        r = []\n",
    "        y_label = set(np.where(y_label==1)[0])\n",
    "        #print(y_preds[idx])\n",
    "        preds = np.where(y_preds[idx]>0.5)[0]\n",
    "        for i in range(depth):\n",
    "            if len(preds)>i:\n",
    "                if preds[i] in y_label:\n",
    "                    tp+=1\n",
    "            p.append(tp/(min(i,len(preds))+1))\n",
    "            r.append(tp/max(len(y_label),1))\n",
    "    \n",
    "    \n",
    "        level_index = []\n",
    "        for idx,level in enumerate(levels):\n",
    "            \n",
    "            if p[level-1]+r[level-1]>0:\n",
    "                level_index.append(2/((1/p[level-1])+(2/r[level-1])))\n",
    "            else:\n",
    "                level_index.append(0)\n",
    "        #print('k',level_index)\n",
    "        \n",
    "        f1.append(level_index)\n",
    "    f1 = np.array(f1)\n",
    "    \n",
    "    print('F1',np.mean(f1,axis=0))\n",
    "    \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadratic programming algorithms largescale model predictive control Quadratic programming QP methods important element application\n"
     ]
    }
   ],
   "source": [
    "sentence_re = r'''(?x)      # set flag to allow verbose regexps\n",
    "            (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "          | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "          | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "          | \\.\\.\\.              # ellipsis\n",
    "          | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "        '''\n",
    "text_toc = nltk.regexp_tokenize(text, sentence_re)\n",
    "candidates = get_kp(text)\n",
    "positions = find_answer(text_toc,references[-1]['KeyPhrases'])\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidates = []\n",
    "references = []\n",
    "\n",
    "for file in files[:10]:\n",
    "    with open(data+file, 'r') as in_file:\n",
    "\n",
    "        \n",
    "        text = in_file.read()\n",
    "        candidates.append({'url':file,\n",
    "                            'KeyPhrases':get_kp(text)})\n",
    "    \n",
    "    name = file.split('.')[0]\n",
    "    with open(keys+name+'.key', 'r') as in_file:\n",
    "\n",
    "        can = in_file.readlines()\n",
    "        can = [line.rstrip('\\n').split() for line in can]\n",
    "        references.append({'url':file,\n",
    "                            'KeyPhrases':can})\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "with open('result.json', 'w') as out_file:\n",
    "    for candidate in candidates:\n",
    "        json.dump(candidate, out_file)\n",
    "        out_file.write('\\n')\n",
    "with open('keys.json', 'w') as out_file:\n",
    "    for ref in references:\n",
    "        json.dump(ref, out_file)\n",
    "        out_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1526,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260 27\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "max_kp = 0\n",
    "min_len =1e100\n",
    "all_reps = []\n",
    "key_positions = []\n",
    "ref_positions = []\n",
    "for file in files[:150]:\n",
    "    with open(data+file, 'r') as in_file:\n",
    "        text = in_file.read()\n",
    "        txt = remove_punctuation(text)\n",
    "        text_toc = nltk.regexp_tokenize(txt, sentence_re)\n",
    "        kps,pos,pos_set = get_kp(text_toc)\n",
    "        #print(pos)\n",
    "        key_positions.append(pos_set)\n",
    "        idx = [tok in model2.wv.vocab for tok in text_toc]\n",
    "        rep = np.zeros((len(text_toc),300),dtype=float)\n",
    "        rep[idx] = model2[np.array(text_toc)[idx]]  \n",
    "        all_reps.append(rep)\n",
    "        max_len = max(max_len,len(text_toc))\n",
    "        min_len = min(min_len,len(text_toc))\n",
    "        max_kp = max(max_kp,len(pos_set))\n",
    "    \n",
    "    name = file.split('.')[0]\n",
    "    with open(keys+name+'.key', 'r') as in_file:\n",
    "\n",
    "        can = in_file.readlines()\n",
    "        can = [line.rstrip('\\n').split() for line in can]\n",
    "        ref_pos,ref_set = find_positions(text_toc,can)\n",
    "        ref_positions.append(ref_set)\n",
    "print(max_len,min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1402,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_positions(text_toc,can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1527,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rep = []\n",
    "for rep in all_reps:\n",
    "    \n",
    "    new_rep.append(tf.pad(rep,[[0,max_len-rep.shape[0]],[0,0]]))  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1415,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "masking_layer = tf.keras.layers.Masking()\n",
    "unmasked_embedding = tf.cast(new_rep, tf.float32)\n",
    "\n",
    "masked_embedding = masking_layer(unmasked_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1528,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_label = []\n",
    "start_pos = []\n",
    "end_pos = []\n",
    "final_positions = []\n",
    "pos_mask = []\n",
    "final_kp_list = []\n",
    "for idx,kp in enumerate(key_positions):\n",
    "    \n",
    "    kp = list(kp)\n",
    "    y_val = [1 if key in ref_positions[idx] else 0 for key in kp]\n",
    "    y_val.extend([0]*(max_kp-len(kp)))\n",
    "    start =[key[0]-1 for key in kp]\n",
    "    end = [key[1]-1 for key in kp]\n",
    "    final_positions.append(tf.pad([start,end],[[0,0],[0,max_kp-len(start)]]))\n",
    "    pos_mask.append([True]*len(start)+[False]*(max_kp-len(start)))\n",
    "    y_label.append(y_val)\n",
    "    final_kp_list.append(kp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1412,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 1412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(key_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPE(object):\n",
    "    \n",
    "    def __init__(self,max_sentence_len,max_keyphrases):\n",
    "        \n",
    "        self.max_sentence_len = max_sentence_len\n",
    "        self.max_keyphrases = max_keyphrases\n",
    "        \n",
    "        \n",
    "        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))\n",
    "        \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNextractor(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RNNextractor, self).__init__()\n",
    "        self.bilstm1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100,\n",
    "                                                                         return_sequences=True),\n",
    "                                                     merge_mode=None,\n",
    "                                                    input_shape=(300,204,))\n",
    "        self.bilstm2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(5,\n",
    "                                                                         return_sequences=True),\n",
    "                                                     merge_mode='ave',\n",
    "                                                     input_shape=(48,100*4))\n",
    "        self.dense = tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "        \n",
    "    def call(self,inputs):\n",
    "                                                    \n",
    "        x = self.bilstm1(inputs[0])\n",
    "        print(x[0].shape)\n",
    "        #print(inputs[1])\n",
    "        mask_start = inputs[1][0]\n",
    "        #mask_start[mask_start>0] = mask_start[mask_start>0]-1\n",
    "        mask_end = inputs[1][1]\n",
    "        #mask_end[mask_end>0] = mask_end[mask_end>0]-1\n",
    "        print(mask_end.shape)\n",
    "        start_rep_fr = tf.gather(x[0],mask_start)\n",
    "        start_rep_bk = tf.gather(x[1],mask_start)\n",
    "        end_rep_fr = tf.gather(x[0],mask_end)\n",
    "        end_rep_bk = tf.gather(x[0],mask_end)\n",
    "        \n",
    "        print(start_rep_bk.shape)\n",
    "        span_fe_diff_fr = start_rep_fr-end_rep_fr\n",
    "        span_fe_prod_fr = tf.math.multiply(start_rep_fr,end_rep_fr)\n",
    "        span_fe_diff_bk = start_rep_bk-end_rep_bk\n",
    "        span_fe_prod_bk = tf.math.multiply(start_rep_bk,end_rep_bk)\n",
    "        print(span_fe_diff_bk.shape)\n",
    "        span_fe = tf.concat([start_rep_fr,\n",
    "                             end_rep_fr,\n",
    "                             start_rep_bk,\n",
    "                             end_rep_bk,\n",
    "                             span_fe_diff_fr,\n",
    "                             span_fe_diff_bk,\n",
    "                             span_fe_prod_fr,\n",
    "                             span_fe_prod_bk\n",
    "                            ],2)\n",
    "        \n",
    "        print(span_fe.shape)\n",
    "        x = self.bilstm2(span_fe)\n",
    "        x =  self.dense(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1202,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = RNNextractor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1203,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tf.random.normal([10,5])\n",
    "pos = tf.constant([[1,5,8],[2,6,9]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1529,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.transpose(new_rep,perm=[0,1,2])\n",
    "x_pos = tf.stack(final_positions)\n",
    "#x_pos = tf.tile(tf.expand_dims(x_pos,-1),[1,1,1,100])\n",
    "y_train = tf.stack(y_label)\n",
    "y_train = tf.cast(y_train,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([20, 2, 48])"
      ]
     },
     "execution_count": 1259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack(final_positions).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer rn_nextractor_72 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "(204, 1, 100)\n",
      "(48,)\n",
      "(48, 1, 100)\n",
      "(48, 1, 100)\n",
      "(48, 1, 800)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([48, 1, 1])"
      ]
     },
     "execution_count": 1205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = extractor([x_train[:,0:1,:],x_pos[:,:,0,0]])\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.        ,  0.        , -1.        ,  0.00121441], dtype=float32)>"
      ]
     },
     "execution_count": 1208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(res[:,0,0]-y_train[0],np.where(pos_mask[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1531,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([150, 77])"
      ]
     },
     "execution_count": 1531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1560,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ip1 = tf.keras.layers.Input(shape=(260,300))\n",
    "mask = tf.keras.layers.Masking(mask_value=0.0)(ip1)\n",
    "#custom_mask = tf.keras.layers.Input(shape=(100,))\n",
    "bilstm1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100,\n",
    "                                                      return_sequences=True),\n",
    "                                                     merge_mode=None)(mask)\n",
    "ip2 = tf.keras.layers.Input(shape=(2,77),dtype='int32')\n",
    "mask_start = ip2[0][0]\n",
    "mask_end = ip2[0][1]\n",
    "\n",
    "start_rep_fr = tf.gather(bilstm1[0],mask_start,axis=1)\n",
    "start_rep_bk = tf.gather(bilstm1[1],mask_start,axis=1)\n",
    "end_rep_fr = tf.gather(bilstm1[0],mask_end,axis=1)\n",
    "end_rep_bk = tf.gather(bilstm1[0],mask_end,axis=1)\n",
    "\n",
    "\n",
    "span_fe_diff_fr = start_rep_fr-end_rep_fr\n",
    "span_fe_prod_fr = tf.math.multiply(start_rep_fr,end_rep_fr)\n",
    "span_fe_diff_bk = start_rep_bk-end_rep_bk\n",
    "span_fe_prod_bk = tf.math.multiply(start_rep_bk,end_rep_bk)\n",
    "\n",
    "\n",
    "span_fe = tf.keras.layers.concatenate([start_rep_fr,\n",
    "                     end_rep_fr,\n",
    "                     start_rep_bk,\n",
    "                     end_rep_bk,\n",
    "                     span_fe_diff_fr,\n",
    "                     span_fe_diff_bk,\n",
    "                     span_fe_prod_fr,\n",
    "                     span_fe_prod_bk\n",
    "                    ],2)\n",
    "bilstm2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(5,return_sequences=True),\n",
    "                                         merge_mode='ave',\n",
    "                                         input_shape=(48,100*4))(span_fe)\n",
    "output = tf.keras.layers.Dense(1,activation='sigmoid',input_shape=(5,))(bilstm2)\n",
    " \n",
    "#output = RNNextractor()([ip1,ip2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1561,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kpe_model = tf.keras.models.Model(inputs=[ip1,ip2], outputs=output)\n",
    "\n",
    "# model = tf.keras.Sequential(\n",
    "#     [ \n",
    "#         RNNextractor()\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(\n",
    "#     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     optimizer=tf.keras.optimizers.Adam(),\n",
    "#     metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# model.fit([x_train[:,0:1,:],x_pos[:,:,0,0]], y_train[0], batch_size=1, epochs=1)\n",
    "# print(model.summary())\n",
    "\n",
    "\n",
    "# # Test the model\n",
    "# #model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1562,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_58 (InputLayer)           [(None, 260, 300)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_59 (InputLayer)           [(None, 2, 77)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_14 (Masking)            (None, 260, 300)     0           input_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_62 (T [(2, 77)]            0           input_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_64 (T [(2, 77)]            0           input_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_238 (Bidirectiona [(None, 260, 100), ( 320800      masking_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_63 (T [(77,)]              0           tf_op_layer_strided_slice_62[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_65 (T [(77,)]              0           tf_op_layer_strided_slice_64[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_GatherV2_75 (Tensor [(None, 77, 100)]    0           bidirectional_238[0][0]          \n",
      "                                                                 tf_op_layer_strided_slice_63[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_GatherV2_77 (Tensor [(None, 77, 100)]    0           bidirectional_238[0][0]          \n",
      "                                                                 tf_op_layer_strided_slice_65[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_GatherV2_76 (Tensor [(None, 77, 100)]    0           bidirectional_238[0][1]          \n",
      "                                                                 tf_op_layer_strided_slice_63[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_GatherV2_78 (Tensor [(None, 77, 100)]    0           bidirectional_238[0][0]          \n",
      "                                                                 tf_op_layer_strided_slice_65[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_36 (TensorFlowO [(None, 77, 100)]    0           tf_op_layer_GatherV2_75[0][0]    \n",
      "                                                                 tf_op_layer_GatherV2_77[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_37 (TensorFlowO [(None, 77, 100)]    0           tf_op_layer_GatherV2_76[0][0]    \n",
      "                                                                 tf_op_layer_GatherV2_78[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_36 (TensorFlowO [(None, 77, 100)]    0           tf_op_layer_GatherV2_75[0][0]    \n",
      "                                                                 tf_op_layer_GatherV2_77[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_37 (TensorFlowO [(None, 77, 100)]    0           tf_op_layer_GatherV2_76[0][0]    \n",
      "                                                                 tf_op_layer_GatherV2_78[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 77, 800)      0           tf_op_layer_GatherV2_75[0][0]    \n",
      "                                                                 tf_op_layer_GatherV2_77[0][0]    \n",
      "                                                                 tf_op_layer_GatherV2_76[0][0]    \n",
      "                                                                 tf_op_layer_GatherV2_78[0][0]    \n",
      "                                                                 tf_op_layer_sub_36[0][0]         \n",
      "                                                                 tf_op_layer_sub_37[0][0]         \n",
      "                                                                 tf_op_layer_Mul_36[0][0]         \n",
      "                                                                 tf_op_layer_Mul_37[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_239 (Bidirectiona (None, 77, 5)        32240       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_134 (Dense)               (None, 77, 1)        6           bidirectional_239[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 353,046\n",
      "Trainable params: 353,046\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(kpe_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1585,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adamax(learning_rate=0.1)\n",
    "kpe_model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1362,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = tf.cast(pos_mask,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72 samples, validate on 18 samples\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 18s 252ms/sample - loss: 27.0651 - accuracy: 0.6806 - val_loss: 26.6658 - val_accuracy: 0.9206\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 3s 47ms/sample - loss: 26.0556 - accuracy: 0.9215 - val_loss: 26.2063 - val_accuracy: 0.9206\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 3s 46ms/sample - loss: 25.1656 - accuracy: 0.9215 - val_loss: 25.9417 - val_accuracy: 0.9206\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 3s 48ms/sample - loss: 24.3485 - accuracy: 0.8685 - val_loss: 26.0617 - val_accuracy: 0.8600\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 3s 46ms/sample - loss: 23.8194 - accuracy: 0.7011 - val_loss: 26.0178 - val_accuracy: 0.8672\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 3s 48ms/sample - loss: 23.9166 - accuracy: 0.7673 - val_loss: 26.2326 - val_accuracy: 0.8506\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 3s 47ms/sample - loss: 23.9957 - accuracy: 0.6856 - val_loss: 26.3939 - val_accuracy: 0.8326\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 4s 49ms/sample - loss: 23.7769 - accuracy: 0.5177 - val_loss: 26.4927 - val_accuracy: 0.8326\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 3s 48ms/sample - loss: 24.0073 - accuracy: 0.5332 - val_loss: 26.6185 - val_accuracy: 0.8326\n",
      "Epoch 10/10\n"
     ]
    }
   ],
   "source": [
    "kpe_model.fit([x_train[:90],x_pos[:90]], y_train[:90], \n",
    "              batch_size=24,epochs=10,\n",
    "              use_multiprocessing=True,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = kpe_model.evaluate([x_train[90:],x_pos[90:]], y_train[90:], batch_size=8)\n",
    "#print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = kpe_model.predict([x_train[90:],x_pos[90:]])\n",
    "print(\"predictions shape:\", predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_f1(y_train[90:],predictions,30,[5,10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
